"""
Step 3: Load JSON data to database using existing SQL ETL files.
Uses proper dimensional schema from dags/sql/ddl/ and ETL from dags/sql/etl/
"""

import json
import sys
from pathlib import Path

from airflow.providers.postgres.hooks.postgres import PostgresHook

# Add the src directory to the Python path
_dag_file_path = Path(__file__).resolve()
_project_root = _dag_file_path.parent.parent.parent
sys.path.append(str(_project_root / "src"))

# Configuration
PROJECT_ROOT = _project_root
RAW_STOCKS_DIR = PROJECT_ROOT / "dags" / "raw_data" / "stocks"
RAW_EXCHANGE_DIR = PROJECT_ROOT / "dags" / "raw_data" / "exchange"
SQL_DDL_DIR = PROJECT_ROOT / "dags" / "sql" / "ddl"
SQL_ETL_DIR = PROJECT_ROOT / "dags" / "sql" / "etl"
POSTGRES_CONN_ID = "postgres_default"


def load_json_to_db() -> dict:
    """Step 3: Load JSON data to database using existing schema."""
    print("üíæ Loading JSON data to database...")

    postgres_hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)

    # Ensure all schema exists first
    print("üîß Ensuring complete database schema exists...")
    ddl_files = ["001_create_dimensions.sql", "002_create_facts.sql", "003_create_views.sql", "004_create_indexes.sql"]

    for ddl_file in ddl_files:
        ddl_path = SQL_DDL_DIR / ddl_file
        if ddl_path.exists():
            print(f"üìÑ Executing {ddl_file}")
            with open(ddl_path, "r") as f:
                postgres_hook.run(f.read())

    stock_records = 0
    exchange_records = 0

    # Load stock data using existing logic
    stock_files = list(RAW_STOCKS_DIR.glob("*.json"))
    print(f"üìÅ Processing {len(stock_files)} stock files...")

    for json_file in stock_files:
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)

            # Process JSON data - adapt to actual structure
            if isinstance(data, list):
                for record in data[:10]:  # Limit for testing
                    if isinstance(record, dict) and "symbol" in record:
                        # Insert into raw_stock_data staging table
                        postgres_hook.run(
                            """
                            INSERT INTO raw_stock_data (
                                symbol, data_date, open_price, high_price, 
                                low_price, close_price, volume, source, created_at
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                            ON CONFLICT (symbol, data_date, source) DO UPDATE SET
                                open_price = EXCLUDED.open_price,
                                close_price = EXCLUDED.close_price,
                                volume = EXCLUDED.volume,
                                updated_at = CURRENT_TIMESTAMP
                        """,
                            parameters=[
                                record.get("symbol", "UNKNOWN"),
                                record.get("date", "2024-01-01"),
                                float(record.get("open", 0.0)),
                                float(record.get("high", 0.0)),
                                float(record.get("low", 0.0)),
                                float(record.get("close", 0.0)),
                                int(record.get("volume", 0)),
                                "alpha_vantage",
                                "now()",
                            ],
                        )
                        stock_records += 1

            print(f"‚úÖ Processed {json_file.name}")

        except Exception as e:
            print(f"‚ö†Ô∏è Error processing {json_file.name}: {e}")

    # Process stock data through dimensional model
    if stock_records > 0:
        print("üîÑ Processing stock data through dimensional model...")
        try:
            # Load dates into dimension
            postgres_hook.run(
                """
                INSERT INTO dim_date (
                    date_value, year, quarter, month, day, 
                    day_of_week, day_of_year, week_of_year, is_weekend
                )
                SELECT DISTINCT
                    data_date,
                    EXTRACT(YEAR FROM data_date),
                    EXTRACT(QUARTER FROM data_date),
                    EXTRACT(MONTH FROM data_date),
                    EXTRACT(DAY FROM data_date),
                    EXTRACT(DOW FROM data_date),
                    EXTRACT(DOY FROM data_date),
                    EXTRACT(WEEK FROM data_date),
                    CASE WHEN EXTRACT(DOW FROM data_date) IN (0, 6) THEN TRUE ELSE FALSE END
                FROM raw_stock_data
                WHERE data_date IS NOT NULL
                ON CONFLICT (date_value) DO NOTHING
            """
            )

            # Load into fact table
            postgres_hook.run(
                """
                INSERT INTO fact_stock_prices (
                    stock_id, date_id, opening_price, high_price, 
                    low_price, closing_price, volume, adjusted_close
                )
                SELECT 
                    ds.stock_id, dd.date_id, rsd.open_price, rsd.high_price,
                    rsd.low_price, rsd.close_price, rsd.volume, rsd.close_price
                FROM raw_stock_data rsd
                JOIN dim_stocks ds ON rsd.symbol = ds.symbol
                JOIN dim_date dd ON rsd.data_date = dd.date_value
                WHERE rsd.open_price IS NOT NULL AND rsd.close_price IS NOT NULL
                ON CONFLICT (stock_id, date_id) DO UPDATE SET
                    opening_price = EXCLUDED.opening_price,
                    closing_price = EXCLUDED.closing_price,
                    volume = EXCLUDED.volume,
                    updated_at = CURRENT_TIMESTAMP
            """
            )

        except Exception as e:
            print(f"‚ö†Ô∏è Error in dimensional processing: {e}")

    # Load exchange data (simplified for now)
    exchange_files = list(RAW_EXCHANGE_DIR.glob("*.json"))
    print(f"üìÅ Processing {len(exchange_files)} exchange files...")

    for json_file in exchange_files:
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)

            # Process exchange data - adapt to actual structure
            if isinstance(data, list):
                for record in data[:10]:  # Limit for testing
                    if isinstance(record, dict):
                        # Insert into raw currency data if table exists
                        try:
                            postgres_hook.run(
                                """
                                INSERT INTO raw_currency_data (
                                    from_currency, to_currency, exchange_rate, 
                                    rate_date, source, created_at
                                ) VALUES (%s, %s, %s, %s, %s, %s)
                                ON CONFLICT (from_currency, to_currency, rate_date, source) 
                                DO UPDATE SET
                                    exchange_rate = EXCLUDED.exchange_rate,
                                    updated_at = CURRENT_TIMESTAMP
                            """,
                                parameters=[
                                    record.get("from", "USD"),
                                    record.get("to", "GBP"),
                                    float(record.get("rate", 1.0)),
                                    record.get("date", "2024-01-01"),
                                    "alpha_vantage",
                                    "now()",
                                ],
                            )
                            exchange_records += 1
                        except Exception as e:
                            print(f"‚ö†Ô∏è Exchange data table not ready: {e}")

            print(f"‚úÖ Processed {json_file.name}")

        except Exception as e:
            print(f"‚ö†Ô∏è Error processing {json_file.name}: {e}")

    print(f"üéâ Loaded {stock_records} stock records and {exchange_records} exchange records")

    return {
        "stock_records_loaded": stock_records,
        "exchange_records_loaded": exchange_records,
        "schema_initialized": True,
    }
