# Airflow 3 Setup and DAG Architecture Guide

## Overview

This document provides comprehensive guidance for Apache Airflow 3 setup, configuration, and DAG implementation within the ticker-converter project. It details both the Airflow management system and modern ETL pipeline orchestration architecture using Airflow 3 features.

## Table of Contents

1. [System Architecture](#system-architecture)
2. [Airflow 3 Installation and Setup](#airflow-3-installation-and-setup)
3. [DAG Functions and Architecture](#dag-functions-and-architecture)
4. [Airflow 3 Implementation Examples](#airflow-3-implementation-examples)
5. [Management Components](#management-components)
6. [Configuration and Best Practices](#configuration-and-best-practices)
7. [Deployment and Testing](#deployment-and-testing)

## System Architecture

### Management Flow
```
Makefile Commands â†’ CLI (cli_ingestion.py) â†’ Orchestrator â†’ AirflowManager â†’ Airflow 3 Services
```

### ETL Pipeline Flow
```
Raw Data Sources â†’ Airflow 3 DAGs â†’ SQL Transformations â†’ Data Warehouse â†’ Analytics
     â”‚                  â”‚              â”‚                    â”‚            â”‚
Alpha Vantage API   daily_etl_dag    PostgreSQL        Dimensional     Business
Currency APIs       test_etl_dag     Operators         Model          Intelligence
```

## Airflow 3 Installation and Setup

### Prerequisites

**System Requirements**:
- Apache Airflow 3.0+
- PostgreSQL 14+
- Python 3.9+
- Required Airflow 3 providers

### Airflow 3 Provider Installation
```bash
# Core Airflow 3 installation
pip install apache-airflow==3.0.0

# Essential providers for Airflow 3
pip install apache-airflow-providers-postgres>=5.10.0
pip install apache-airflow-providers-common-sql>=1.14.0
pip install apache-airflow-providers-standard>=0.0.1

# Optional monitoring providers
pip install apache-airflow-providers-email>=1.3.0
pip install apache-airflow-providers-slack>=8.6.0

# Development and testing
pip install apache-airflow-providers-celery>=3.6.0  # For distributed execution
```

### Airflow 3 Configuration Updates

#### Core Configuration (`airflow.cfg`)
```ini
[core]
# DAGs configuration with Airflow 3 optimizations
dags_folder = /opt/airflow/dags
base_log_folder = /opt/airflow/logs
remote_logging = False
log_level = INFO

# New in Airflow 3: Enhanced DAG processing
dag_discovery_safe_mode = True
dag_ignore_file_syntax = \.airflowignore|\.gitignore
enable_xcom_pickling = False  # Security improvement in Airflow 3

# Executor configuration
executor = LocalExecutor  # Or CeleryExecutor for distributed

# Security enhancements in Airflow 3
fernet_key = your_fernet_key_here
secret_key = your_secret_key_here  # New in Airflow 3

[database]
# Use PostgreSQL for Airflow metadata (required for Airflow 3)
sql_alchemy_conn = postgresql://airflow_user:airflow_password@localhost:5432/airflow_db

# Enhanced connection pooling in Airflow 3
sql_alchemy_pool_size = 10
sql_alchemy_max_overflow = 20
sql_alchemy_pool_pre_ping = True
sql_alchemy_pool_recycle = 3600
sql_alchemy_engine_args = {"pool_reset_on_return": "commit"}

[scheduler]
# Improved scheduler performance in Airflow 3
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5
parsing_processes = 2  # New: parallel DAG parsing
dag_dir_list_interval = 300

[webserver]
# Enhanced security in Airflow 3
web_server_host = 0.0.0.0
web_server_port = 8080
secret_key = your_secret_key_here
session_timeout_minutes = 43200

# New RBAC features in Airflow 3
rbac = True
auth_backends = airflow.auth.backends.session
```

## DAG Functions and Architecture

### SQL-Centric ETL Pipeline Design

**Architecture Philosophy**: The ticker-converter project implements a **SQL-first approach** that leverages PostgreSQL for all analytical operations while minimizing Python business logic. This design pattern provides:

- **SQL Operators Only**: All transformations use `PostgresOperator`/`SQLExecuteQueryOperator`
- **External SQL Files**: Business logic stored in version-controlled `.sql` files
- **Minimal Python Logic**: Python operators exclusively for API data ingestion
- **Linear Dependencies**: Simple task flow without complex branching
- **Idempotent Operations**: All tasks safely re-runnable

### Primary DAG: `ticker_converter_daily_etl` (Airflow 3)

**Purpose**: Orchestrates complete daily ETL pipeline for stock market data and currency conversion using Airflow 3 features and SQL-centric architecture

**Core Pipeline Flow**:
```
API Ingestion â†’ Dimension Loading â†’ SQL Transformations â†’ Data Quality â†’ Cleanup
     â†“                â†“                    â†“               â†“           â†“
Python Tasks      SQL Files         PostgreSQL        SQL Checks  SQL Cleanup
(API calls)    (dimension.sql)    (fact tables)   (validation.sql) (retention.sql)
```

**Core Functions**:
1. **Data Ingestion**: Python tasks for API calls only (NYSE stocks, USD/GBP rates)
2. **Dimension Management**: SQL-based reference data maintenance via external `.sql` files
3. **SQL Transformations**: PostgreSQL-native data processing for star schema population
4. **Quality Assurance**: SQL-based validation of business rules and data integrity
5. **Data Maintenance**: SQL-driven cleanup according to retention policies

**Schedule**: Daily at 6:00 PM EST (`"0 18 * * 1-5"`) - weekdays only for market data

**Target Data Sources**:
- **Alpha Vantage API**: Magnificent Seven stock prices (AAPL, GOOGL, AMZN, MSFT, TSLA, META, NVDA)
- **ExchangeRate-API**: USD/GBP currency conversion rates

**SQL File Organization**:
```
sql/etl/
â”œâ”€â”€ load_dimensions.sql       # Dimension table population
â”œâ”€â”€ daily_transform.sql       # Fact table transformations  
â”œâ”€â”€ data_quality_checks.sql   # Business rule validation
â””â”€â”€ cleanup_old_data.sql      # Data retention management
```

### Secondary DAG: `test_etl_dag`

**Purpose**: Development and testing pipeline for ETL workflow validation

**Functions**:
- Test API integrations in isolated environment
- Validate SQL transformations with sample data
- Debug pipeline issues without affecting production data

## Airflow 3 Implementation Examples

### 1. Modern DAG Definition with Airflow 3

```python
"""
Airflow 3 DAG for daily stock data ETL pipeline.
Demonstrates modern Airflow 3 patterns and operators.
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.models.param import Param

# Airflow 3 imports - updated provider structure
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.providers.standard.operators.empty import EmptyOperator
from airflow.providers.standard.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

# Airflow 3 decorators and utilities
from airflow.decorators import task, task_group
from airflow.utils.task_group import TaskGroup
from airflow.utils.trigger_rule import TriggerRule

# Modern Airflow 3 DAG configuration with params
dag = DAG(
    dag_id="ticker_converter_daily_etl_v3",
    description="Daily ETL pipeline using Airflow 3 features",
    
    # Airflow 3 enhanced parameters
    params={
        "data_source": Param(
            default="alpha_vantage",
            enum=["alpha_vantage", "yahoo_finance"],
            description="Data source for stock prices"
        ),
        "days_back": Param(
            default=5,
            type="integer",
            minimum=1,
            maximum=30,
            description="Number of days to fetch historical data"
        ),
        "enable_quality_checks": Param(
            default=True,
            type="boolean",
            description="Enable data quality validation"
        )
    },
    
    # Modern scheduling with Airflow 3
    schedule="@daily",  # Airflow 3 preferred syntax
    start_date=datetime(2024, 1, 1),
    catchup=False,
    max_active_runs=1,
    
    # Airflow 3 enhanced default args
    default_args={
        "owner": "ticker-converter",
        "depends_on_past": False,
        "email_on_failure": False,
        "email_on_retry": False,
        "retries": 2,
        "retry_delay": timedelta(minutes=5),
        "retry_exponential_backoff": True,  # New in Airflow 3
        "max_retry_delay": timedelta(minutes=30),  # New in Airflow 3
    },
    
    # Airflow 3 tagging and documentation
    tags=["ticker-converter", "etl", "stocks", "airflow-3"],
    doc_md=__doc__,
    
    # Airflow 3 render template configuration
    render_template_as_native_obj=True,  # Improved templating
)
```

### 2. Airflow 3 TaskFlow API Examples

```python
# Airflow 3 TaskFlow API with improved typing
@task(task_id="extract_stock_data_v3")
def extract_stock_prices_v3(
    days_back: int = 5,
    data_source: str = "alpha_vantage"
) -> dict[str, int]:
    """
    Extract stock prices using Airflow 3 TaskFlow API.
    
    Returns:
        Dictionary with extraction metrics
    """
    from ticker_converter.data_ingestion.nyse_fetcher import NYSEDataFetcher
    from ticker_converter.data_ingestion.database_manager import DatabaseManager
    
    # Initialize components
    db_manager = DatabaseManager()
    fetcher = NYSEDataFetcher()
    
    try:
        # Use DAG params in Airflow 3
        all_records = fetcher.fetch_and_prepare_all_data(days_back=days_back)
        
        if all_records:
            db_manager.insert_stock_data(all_records)
            return {
                "records_inserted": len(all_records),
                "data_source": data_source,
                "status": "success"
            }
        else:
            return {
                "records_inserted": 0,
                "data_source": data_source,
                "status": "no_data"
            }
            
    except Exception as e:
        # Airflow 3 enhanced error handling
        raise ValueError(f"Stock extraction failed: {str(e)}") from e

@task(task_id="extract_currency_data_v3")
def extract_exchange_rates_v3(days_back: int = 5) -> dict[str, int]:
    """
    Extract exchange rates using Airflow 3 TaskFlow API.
    """
    from ticker_converter.data_ingestion.currency_fetcher import CurrencyDataFetcher
    from ticker_converter.data_ingestion.database_manager import DatabaseManager
    
    db_manager = DatabaseManager()
    fetcher = CurrencyDataFetcher()
    
    try:
        all_records = fetcher.fetch_and_prepare_fx_data(days_back=days_back)
        
        if all_records:
            db_manager.insert_currency_data(all_records)
            return {
                "records_inserted": len(all_records),
                "status": "success"
            }
        else:
            return {
                "records_inserted": 0,
                "status": "no_data"
            }
            
    except Exception as e:
        raise ValueError(f"Currency extraction failed: {str(e)}") from e
```

### 3. SQL-Centric Task Implementation (From Project Documentation)

```python
# Task structure following ticker-converter SQL-first architecture
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.standard.operators.python import PythonOperator

# 1. Data Ingestion (Python) - API calls only
fetch_stock_data = PythonOperator(
    task_id='fetch_stock_data',
    python_callable=fetch_nyse_stock_data,
    dag=dag
)

fetch_currency_rates = PythonOperator(
    task_id='fetch_currency_rates', 
    python_callable=fetch_usd_gbp_rates,
    dag=dag
)

# 2. Dimension Loading (SQL) - External SQL files
load_dimensions = PostgresOperator(
    task_id='load_dimensions',
    postgres_conn_id='postgres_default',
    sql='sql/etl/load_dimensions.sql',  # External SQL file
    dag=dag
)

# 3. Fact Table Transformation (SQL) - PostgreSQL native
transform_to_warehouse = PostgresOperator(
    task_id='transform_to_warehouse',
    postgres_conn_id='postgres_default', 
    sql='sql/etl/daily_transform.sql',  # Star schema population
    dag=dag
)

# 4. Data Quality Validation (SQL) - Business rules
data_quality_check = PostgresOperator(
    task_id='data_quality_check',
    postgres_conn_id='postgres_default',
    sql='sql/etl/data_quality_checks.sql',  # OHLC validation, completeness
    dag=dag
)

# 5. Data Cleanup (SQL) - Retention management
cleanup_staging = PostgresOperator(
    task_id='cleanup_staging',
    postgres_conn_id='postgres_default',
    sql='sql/etl/cleanup_old_data.sql',  # 7-day retention policy
    dag=dag
)

# Linear dependency chain - SQL-centric flow
fetch_stock_data >> load_dimensions
fetch_currency_rates >> load_dimensions  
load_dimensions >> transform_to_warehouse
transform_to_warehouse >> data_quality_check
data_quality_check >> cleanup_staging
```

**Key SQL Files Referenced**:

#### `sql/etl/load_dimensions.sql`
```sql
-- Dimension table population with conflict resolution
INSERT INTO dim_stocks (symbol, company_name, sector, exchange)
SELECT DISTINCT 
    symbol,
    CASE 
        WHEN symbol = 'AAPL' THEN 'Apple Inc.'
        WHEN symbol = 'MSFT' THEN 'Microsoft Corporation'
        -- ... Magnificent Seven mappings
    END as company_name,
    -- Sector classification logic
FROM staging_stock_data
ON CONFLICT (symbol) DO NOTHING;
```

#### `sql/etl/daily_transform.sql` 
```sql
-- Star schema fact table population with Airflow templating
INSERT INTO fact_stock_prices (date_id, stock_id, open_usd, high_usd, low_usd, close_usd, volume)
SELECT 
    d.date_id,
    s.stock_id,
    st.open_usd,
    st.high_usd, 
    st.low_usd,
    st.close_usd,
    st.volume
FROM staging_stock_data st
JOIN dim_dates d ON st.date = d.date
JOIN dim_stocks s ON st.symbol = s.symbol
WHERE st.date = '{{ ds }}'  -- Airflow template for execution date
ON CONFLICT (date_id, stock_id) DO UPDATE SET
    -- Update existing records idempotently
```

#### `sql/etl/data_quality_checks.sql`
```sql
-- Business rule validation with PostgreSQL procedures
DO $$ 
DECLARE
    expected_stocks INTEGER := 7;  -- Magnificent Seven validation
    actual_stocks INTEGER;
BEGIN
    SELECT COUNT(DISTINCT s.symbol)
    INTO actual_stocks
    FROM fact_stock_prices p
    JOIN dim_stocks s ON p.stock_id = s.stock_id
    WHERE date = '{{ ds }}';

    IF actual_stocks < expected_stocks THEN
        RAISE EXCEPTION 'Data quality check failed: Expected % stocks, found %', 
            expected_stocks, actual_stocks;
    END IF;
END $$;

-- OHLC consistency validation
-- Currency rate completeness checks
-- Trading day validation logic
```

### 4. Airflow 3 DAG Assembly with Dependencies

```python
# Airflow 3 DAG assembly using TaskGroups
with dag:
    # Start and end markers
    start_pipeline = EmptyOperator(
        task_id="start_etl_pipeline",
        
        # Airflow 3 enhanced documentation
        doc_md="""
        ## ETL Pipeline Start
        
        This task marks the beginning of the daily ETL pipeline.
        
        ### Pipeline Overview:
        1. Load dimension tables
        2. Extract data from APIs
        3. Transform and load facts
        4. Run quality checks
        5. Clean up old data
        """
    )
    
    end_pipeline = EmptyOperator(
        task_id="end_etl_pipeline",
        
        # Airflow 3 trigger rules
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
        
        doc_md="Pipeline completion marker with enhanced error tolerance."
    )
    
    # Task groups
    dimensions = dimension_loading_group()
    extractions = data_extraction_group()
    transformations = transformation_quality_group()
    
    # Airflow 3 enhanced dependency syntax
    start_pipeline >> dimensions >> extractions >> transformations >> end_pipeline

# Airflow 3 DAG-level validation
def validate_dag_params(context):
    """
    Validate DAG parameters using Airflow 3 features.
    """
    params = context["params"]
    
    if params["days_back"] > 30:
        raise ValueError("days_back cannot exceed 30 days")
    
    if params["data_source"] not in ["alpha_vantage", "yahoo_finance"]:
        raise ValueError("Invalid data source specified")
    
    return True

# Add validation task
validate_params = PythonOperator(
    task_id="validate_dag_parameters",
    python_callable=validate_dag_params,
    dag=dag
)

# Insert validation at the beginning
start_pipeline >> validate_params >> dimensions
```

### 5. Airflow 3 Enhanced SQL Operations

```python
# Airflow 3 SQL operations with advanced features
class AdvancedSQLOperations:
    """
    Demonstrates Airflow 3 advanced SQL patterns.
    """
    
    @staticmethod
    def create_dynamic_sql_task(table_name: str, sql_template: str) -> SQLExecuteQueryOperator:
        """
        Create dynamic SQL tasks using Airflow 3 features.
        """
        return SQLExecuteQueryOperator(
            task_id=f"process_{table_name}",
            conn_id="postgres_default",
            sql=sql_template,
            
            # Airflow 3 enhanced parameterization
            parameters={
                "table_name": table_name,
                "execution_date": "{{ ds }}",
                "dag_run_conf": "{{ dag_run.conf }}",  # Access DAG run configuration
                "logical_date": "{{ logical_date }}",  # New in Airflow 3
            },
            
            # Airflow 3 result handling
            do_xcom_push=True,
            return_last=True,
            
            # Airflow 3 hook parameters for connection tuning
            hook_params={
                "options": "-c statement_timeout=600s -c lock_timeout=30s"
            }
        )

# Example: Dynamic table processing
fact_tables = ["fact_stock_prices", "fact_currency_rates", "fact_performance_metrics"]

sql_template = """
    -- Airflow 3 enhanced SQL template
    INSERT INTO {{ params.table_name }} 
    SELECT * FROM staging_{{ params.table_name | replace('fact_', '') }}
    WHERE created_date = '{{ logical_date }}'
    ON CONFLICT DO UPDATE SET 
        updated_at = CURRENT_TIMESTAMP,
        updated_by = 'airflow_3_etl';
        
    -- Return processing metrics
    SELECT 
        '{{ params.table_name }}' as table_name,
        COUNT(*) as rows_processed,
        '{{ logical_date }}' as process_date,
        CURRENT_TIMESTAMP as completion_time;
"""

# Generate dynamic tasks for each fact table
fact_processing_tasks = [
    AdvancedSQLOperations.create_dynamic_sql_task(table, sql_template) 
    for table in fact_tables
]
```

### 6. Airflow 3 Error Handling and Monitoring

```python
# Airflow 3 enhanced error handling patterns
@task(task_id="data_quality_monitor_v3", retries=0)
def enhanced_data_quality_monitor(**context) -> dict:
    """
    Advanced data quality monitoring using Airflow 3 features.
    """
    from airflow.providers.postgres.hooks.postgres import PostgresHook
    from airflow.exceptions import AirflowSkipException, AirflowFailException
    
    # Airflow 3 hook with enhanced connection handling
    hook = PostgresHook(
        postgres_conn_id="postgres_default",
        schema="ticker_converter"
    )
    
    # Data quality checks with Airflow 3 context
    logical_date = context["logical_date"]
    dag_run = context["dag_run"]
    
    quality_checks = [
        {
            "name": "completeness_check",
            "sql": """
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT symbol) as unique_symbols,
                    MIN(data_date) as min_date,
                    MAX(data_date) as max_date
                FROM fact_stock_prices p
                JOIN dim_dates d ON p.date_id = d.date_id
                WHERE d.date = %(target_date)s
            """,
            "min_records": 7,  # Magnificent Seven stocks
            "min_symbols": 7
        },
        {
            "name": "data_freshness_check", 
            "sql": """
                SELECT 
                    EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP - MAX(updated_at)))/3600 as hours_since_update
                FROM fact_stock_prices p
                JOIN dim_dates d ON p.date_id = d.date_id
                WHERE d.date = %(target_date)s
            """,
            "max_hours_old": 24
        }
    ]
    
    results = {}
    
    for check in quality_checks:
        try:
            result = hook.get_first(
                sql=check["sql"],
                parameters={"target_date": logical_date.date()}
            )
            
            results[check["name"]] = result
            
            # Airflow 3 conditional logic
            if check["name"] == "completeness_check":
                total_records, unique_symbols = result[0], result[1]
                
                if total_records < check["min_records"]:
                    raise AirflowFailException(
                        f"Completeness check failed: {total_records} < {check['min_records']}"
                    )
                    
                if unique_symbols < check["min_symbols"]:
                    raise AirflowFailException(
                        f"Symbol diversity check failed: {unique_symbols} < {check['min_symbols']}"
                    )
                    
            elif check["name"] == "data_freshness_check":
                hours_old = result[0] if result[0] else 0
                
                if hours_old > check["max_hours_old"]:
                    # Use Airflow 3 skip exception for non-critical failures
                    raise AirflowSkipException(
                        f"Data is {hours_old} hours old, skipping downstream tasks"
                    )
                    
        except Exception as e:
            results[check["name"]] = {"error": str(e)}
            
            # Re-raise critical errors
            if isinstance(e, (AirflowFailException, AirflowSkipException)):
                raise
            
            # Log and continue for non-critical errors
            context["task_instance"].log.warning(f"Quality check {check['name']} failed: {e}")
    
    return results

# Airflow 3 callback functions with enhanced context
def task_success_callback_v3(context):
    """
    Enhanced success callback using Airflow 3 context.
    """
    task_instance = context["task_instance"]
    logical_date = context["logical_date"]
    dag_run = context["dag_run"]
    
    # Airflow 3 logging improvements
    task_instance.log.info(f"Task {task_instance.task_id} completed successfully")
    task_instance.log.info(f"Logical date: {logical_date}")
    task_instance.log.info(f"DAG run ID: {dag_run.run_id}")
    
    # XCom push with Airflow 3 enhancements
    task_instance.xcom_push(
        key="success_metrics",
        value={
            "completion_time": datetime.now().isoformat(),
            "logical_date": logical_date.isoformat(),
            "duration_seconds": (
                task_instance.end_date - task_instance.start_date
            ).total_seconds()
        }
    )

def task_failure_callback_v3(context):
    """
    Enhanced failure callback with Airflow 3 error context.
    """
    task_instance = context["task_instance"]
    exception = context.get("exception")
    
    # Enhanced error logging in Airflow 3
    error_details = {
        "task_id": task_instance.task_id,
        "dag_id": task_instance.dag_id,
        "execution_date": context["logical_date"].isoformat(),
        "error_message": str(exception) if exception else "Unknown error",
        "retry_number": task_instance.try_number,
        "max_tries": task_instance.max_tries + 1
    }
    
    task_instance.log.error(f"Task failed with details: {error_details}")
    
    # Send alert (placeholder for actual alerting system)
    send_failure_alert_v3(error_details)

def send_failure_alert_v3(error_details: dict):
    """
    Send enhanced failure alerts using Airflow 3 patterns.
    """
    # Implementation would use Airflow 3 notification providers
    pass
```

## Management Components

## Management Components

### Airflow 3 Management System

The management system has been updated to support Airflow 3 features and improved functionality:

### 1. AirflowManager (`src/ticker_converter/data_ingestion/airflow_manager.py`)

**Purpose**: Handles all Apache Airflow 3 lifecycle operations with enhanced capabilities

**Airflow 3 Enhancements**:
- Support for new Airflow 3 CLI commands and options
- Enhanced metadata database management with improved schema
- Better process management with Airflow 3 daemon improvements
- Advanced monitoring using Airflow 3 metrics and health endpoints

**Key Methods Updated for Airflow 3**:

#### Setup Operations
```python
def initialize_airflow_database_v3(self) -> bool:
    """Initialize Airflow 3 metadata database with enhanced features."""
    try:
        # Airflow 3 database initialization with new options
        result = subprocess.run([
            "airflow", "db", "migrate",
            "--show-sql-only", "false",  # New in Airflow 3
            "--from-revision", "auto",   # Enhanced migration control
            "--to-revision", "heads"
        ], capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            logging.info("Airflow 3 database initialized successfully")
            return True
        else:
            logging.error(f"Database initialization failed: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        logging.error("Database initialization timed out")
        return False

def create_admin_user_v3(self) -> bool:
    """Create admin user with Airflow 3 enhanced security."""
    try:
        # Airflow 3 user creation with enhanced RBAC
        result = subprocess.run([
            "airflow", "users", "create",
            "--username", os.getenv("AIRFLOW_ADMIN_USERNAME"),
            "--firstname", os.getenv("AIRFLOW_ADMIN_FIRSTNAME"),
            "--lastname", os.getenv("AIRFLOW_ADMIN_LASTNAME"),
            "--role", "Admin",  # Airflow 3 role-based access
            "--email", os.getenv("AIRFLOW_ADMIN_EMAIL"),
            "--password", os.getenv("AIRFLOW_ADMIN_PASSWORD"),
            "--use-random-password", "false"  # New option in Airflow 3
        ], capture_output=True, text=True)
        
        return result.returncode == 0
        
    except Exception as e:
        logging.error(f"User creation failed: {e}")
        return False

def start_webserver_v3(self, port: int = 8080) -> bool:
    """Start Airflow 3 webserver with enhanced features."""
    try:
        # Airflow 3 webserver with improved options
        cmd = [
            "airflow", "webserver",
            "--port", str(port),
            "--workers", "4",  # Enhanced worker management
            "--worker-timeout", "120",
            "--access-logfile", "-",  # Improved logging in Airflow 3
            "--error-logfile", "-",
            "--daemon"  # Background execution
        ]
        
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        # Enhanced process tracking
        self._webserver_pid = process.pid
        logging.info(f"Airflow 3 webserver started on port {port} (PID: {process.pid})")
        return True
        
    except Exception as e:
        logging.error(f"Failed to start webserver: {e}")
        return False

def start_scheduler_v3(self) -> bool:
    """Start Airflow 3 scheduler with performance improvements."""
    try:
        # Airflow 3 scheduler with enhanced configuration
        cmd = [
            "airflow", "scheduler",
            "--num-runs", "-1",  # Run indefinitely
            "--do-pickle", "false",  # Security improvement
            "--subdir", os.getenv("AIRFLOW_DAGS_FOLDER", "dags"),
            "--daemon"
        ]
        
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        self._scheduler_pid = process.pid
        logging.info(f"Airflow 3 scheduler started (PID: {process.pid})")
        return True
        
    except Exception as e:
        logging.error(f"Failed to start scheduler: {e}")
        return False
```

#### Monitoring Operations
```python
def get_airflow_status_v3(self) -> dict:
    """Get comprehensive Airflow 3 status information."""
    try:
        # Airflow 3 health check endpoint
        health_result = subprocess.run([
            "airflow", "db", "check-migrations"
        ], capture_output=True, text=True)
        
        # Enhanced process detection
        processes = self._get_airflow_processes_v3()
        
        # Airflow 3 configuration check
        config_result = subprocess.run([
            "airflow", "config", "get-value", "core", "executor"
        ], capture_output=True, text=True)
        
        return {
            "database_healthy": health_result.returncode == 0,
            "webserver_running": any(p["name"] == "webserver" for p in processes),
            "scheduler_running": any(p["name"] == "scheduler" for p in processes),
            "executor": config_result.stdout.strip() if config_result.returncode == 0 else "unknown",
            "processes": processes,
            "airflow_version": self._get_airflow_version(),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logging.error(f"Status check failed: {e}")
        return {"error": str(e)}

def _get_airflow_processes_v3(self) -> list[dict]:
    """Enhanced process detection for Airflow 3."""
    try:
        result = subprocess.run(
            ["ps", "aux"], capture_output=True, text=True
        )
        
        processes = []
        for line in result.stdout.split("\n"):
            if "airflow" in line and any(cmd in line for cmd in ["webserver", "scheduler", "worker"]):
                parts = line.split()
                if len(parts) >= 11:
                    # Enhanced process information
                    processes.append({
                        "pid": int(parts[1]),
                        "name": self._extract_airflow_command(line),
                        "cpu_percent": parts[2],
                        "memory_percent": parts[3],
                        "start_time": parts[8],
                        "command": " ".join(parts[10:])
                    })
        
        return processes
        
    except Exception as e:
        logging.error(f"Process detection failed: {e}")
        return []

def _get_airflow_version(self) -> str:
    """Get Airflow version information."""
    try:
        result = subprocess.run([
            "airflow", "version"
        ], capture_output=True, text=True)
        
        return result.stdout.strip() if result.returncode == 0 else "unknown"
        
    except Exception:
        return "unknown"
```

### 2. Orchestrator Integration (`src/ticker_converter/data_ingestion/orchestrator.py`)

**Airflow 3 Enhanced Methods**:
```python
def perform_airflow_setup_v3(self) -> dict:
    """Enhanced Airflow 3 setup orchestration."""
    try:
        airflow_manager = AirflowManager()
        
        setup_steps = [
            ("database_init", airflow_manager.initialize_airflow_database_v3),
            ("user_creation", airflow_manager.create_admin_user_v3),
            ("webserver_start", lambda: airflow_manager.start_webserver_v3()),
            ("scheduler_start", airflow_manager.start_scheduler_v3)
        ]
        
        results = {}
        
        for step_name, step_func in setup_steps:
            try:
                results[step_name] = step_func()
                if not results[step_name]:
                    logging.warning(f"Setup step {step_name} failed")
                    
            except Exception as e:
                results[step_name] = False
                logging.error(f"Setup step {step_name} error: {e}")
        
        # Overall success check
        overall_success = all(results.values())
        
        return {
            "success": overall_success,
            "steps": results,
            "message": "Airflow 3 setup completed" if overall_success else "Setup completed with errors",
            "airflow_version": airflow_manager._get_airflow_version()
        }
        
    except Exception as e:
        logging.error(f"Orchestrated setup failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": "Airflow 3 setup failed"
        }
```

### 3. CLI Commands (`src/ticker_converter/cli_ingestion.py`)

**Airflow 3 Enhanced Commands**:
```python
def airflow_setup_command_v3() -> None:
    """Enhanced Airflow 3 setup command with comprehensive feedback."""
    print("ðŸš€ Starting Airflow 3 Setup...")
    print("=" * 50)
    
    orchestrator = Orchestrator()
    setup_result = orchestrator.perform_airflow_setup_v3()
    
    if setup_result["success"]:
        print("âœ… Airflow 3 setup completed successfully!")
        print(f"ðŸ”§ Version: {setup_result.get('airflow_version', 'Unknown')}")
        print("\nðŸ“‹ Setup Results:")
        
        for step, success in setup_result["steps"].items():
            status = "âœ…" if success else "âŒ"
            print(f"  {status} {step.replace('_', ' ').title()}")
        
        print("\nðŸŒ Access Information:")
        print("  â€¢ Webserver: http://localhost:8080")
        print("  â€¢ Username: admin1@test.local")
        print("  â€¢ Password: test123")
        print("\nðŸ’¡ Next steps:")
        print("  â€¢ Access the Airflow 3 UI to manage DAGs")
        print("  â€¢ Check DAG status with: make airflow-status")
        
    else:
        print("âŒ Airflow 3 setup failed!")
        print(f"Error: {setup_result.get('error', 'Unknown error')}")
        
        if "steps" in setup_result:
            print("\nðŸ“‹ Step Results:")
            for step, success in setup_result["steps"].items():
                status = "âœ…" if success else "âŒ"
                print(f"  {status} {step.replace('_', ' ').title()}")

def airflow_status_command_v3() -> None:
    """Enhanced Airflow 3 status command with detailed information."""
    print("ðŸ” Checking Airflow 3 Status...")
    print("=" * 40)
    
    airflow_manager = AirflowManager()
    status = airflow_manager.get_airflow_status_v3()
    
    if "error" in status:
        print(f"âŒ Status check failed: {status['error']}")
        return
    
    # Overall status
    webserver_status = "ðŸŸ¢ Running" if status["webserver_running"] else "ðŸ”´ Stopped"
    scheduler_status = "ðŸŸ¢ Running" if status["scheduler_running"] else "ðŸ”´ Stopped"
    database_status = "ðŸŸ¢ Healthy" if status["database_healthy"] else "ðŸ”´ Issues"
    
    print(f"ðŸ“Š Airflow 3 Status Overview:")
    print(f"  â€¢ Version: {status.get('airflow_version', 'Unknown')}")
    print(f"  â€¢ Executor: {status.get('executor', 'Unknown')}")
    print(f"  â€¢ Database: {database_status}")
    print(f"  â€¢ Webserver: {webserver_status}")
    print(f"  â€¢ Scheduler: {scheduler_status}")
    
    # Process details
    if status["processes"]:
        print(f"\nðŸ”§ Running Processes:")
        for process in status["processes"]:
            print(f"  â€¢ {process['name']} (PID: {process['pid']}) - "
                  f"CPU: {process['cpu_percent']}%, "
                  f"Memory: {process['memory_percent']}%")
    else:
        print("\nðŸ”§ No Airflow processes running")
    
    print(f"\nâ° Last checked: {status['timestamp']}")
```

### 4. Makefile Integration

**Enhanced Makefile targets for Airflow 3**:
```makefile
# Airflow 3 enhanced targets
.PHONY: airflow-v3 airflow-stop-v3 airflow-status-v3

airflow-v3: ## Setup Airflow 3 with enhanced features
	@echo "ðŸš€ Setting up Airflow 3..."
	python -m ticker_converter.cli_ingestion --airflow-setup-v3

airflow-stop-v3: ## Stop Airflow 3 services gracefully
	@echo "ðŸ›‘ Stopping Airflow 3 services..."
	python -m ticker_converter.cli_ingestion --airflow-teardown-v3

airflow-status-v3: ## Check comprehensive Airflow 3 status
	@echo "ðŸ” Checking Airflow 3 status..."
	python -m ticker_converter.cli_ingestion --airflow-status-v3

airflow-health-v3: ## Run Airflow 3 health checks
	@echo "ðŸ¥ Running Airflow 3 health checks..."
	airflow db check-migrations
	airflow db check
	airflow config list | grep -E "(executor|sql_alchemy_conn)"
```

## Configuration and Best Practices

### Airflow 3 Environment Configuration

#### Environment Variables (`.env` file)
```bash
# Airflow 3 specific configuration
AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
AIRFLOW__CORE__BASE_LOG_FOLDER=/opt/airflow/logs
AIRFLOW__CORE__REMOTE_LOGGING=False
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__FERNET_KEY=your_fernet_key_here
AIRFLOW__CORE__SECRET_KEY=your_secret_key_here  # New in Airflow 3

# Enhanced database configuration for Airflow 3
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://airflow_user:airflow_password@localhost:5432/airflow_db
AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE=10
AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW=20
AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_PRE_PING=True

# Airflow 3 scheduler enhancements
AIRFLOW__SCHEDULER__PARSING_PROCESSES=2
AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=300
AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=0

# Airflow 3 webserver security
AIRFLOW__WEBSERVER__SECRET_KEY=your_webserver_secret_key
AIRFLOW__WEBSERVER__SESSION_TIMEOUT_MINUTES=43200
AIRFLOW__WEBSERVER__RBAC=True

# Application database
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=ticker_converter
POSTGRES_USER=ticker_user
POSTGRES_PASSWORD=your_secure_password

# API Keys
ALPHA_VANTAGE_API_KEY=your_alpha_vantage_key
CURRENCY_API_KEY=your_currency_api_key

# Admin user configuration
AIRFLOW_ADMIN_USERNAME=admin1@test.local
AIRFLOW_ADMIN_PASSWORD=test123
AIRFLOW_ADMIN_EMAIL=admin1@test.local
AIRFLOW_ADMIN_FIRSTNAME=Admin
AIRFLOW_ADMIN_LASTNAME=User
```

### Airflow 3 Connection Management

#### Enhanced Connection Configuration
```python
# Airflow 3 connection configuration with improved security
from airflow.models import Connection
from airflow.settings import Session
from airflow.configuration import conf

def create_postgres_connection_v3():
    """Create PostgreSQL connection with Airflow 3 enhancements."""
    
    # Airflow 3 connection with enhanced security
    connection = Connection(
        conn_id='postgres_default',
        conn_type='postgres',
        description='Main PostgreSQL database for ticker converter',
        host='localhost',
        schema='ticker_converter',
        login='ticker_user',
        password='your_secure_password',
        port=5432,
        
        # Airflow 3 enhanced extra parameters
        extra={
            "sslmode": "prefer",
            "connect_timeout": "10",
            "application_name": "airflow_3_etl",  # Enhanced connection tracking
            "options": "-c statement_timeout=300s",
            "keepalives_idle": "600",
            "keepalives_interval": "30",
            "keepalives_count": "3"
        }
    )
    
    # Airflow 3 session management
    session = Session()
    try:
        # Remove existing connection if present
        existing = session.query(Connection).filter(
            Connection.conn_id == 'postgres_default'
        ).first()
        
        if existing:
            session.delete(existing)
        
        # Add new connection
        session.add(connection)
        session.commit()
        
        print("âœ… PostgreSQL connection created successfully in Airflow 3")
        
    except Exception as e:
        session.rollback()
        print(f"âŒ Failed to create connection: {e}")
        raise
    finally:
        session.close()

# CLI command for connection setup
def setup_airflow_connections_v3():
    """Setup all required connections for Airflow 3."""
    
    # Enhanced CLI connection creation for Airflow 3
    commands = [
        [
            "airflow", "connections", "add", "postgres_default",
            "--conn-type", "postgres",
            "--conn-host", "localhost",
            "--conn-schema", "ticker_converter", 
            "--conn-login", "ticker_user",
            "--conn-password", os.getenv("POSTGRES_PASSWORD"),
            "--conn-port", "5432",
            "--conn-extra", json.dumps({
                "sslmode": "prefer",
                "connect_timeout": "10",
                "application_name": "airflow_3_etl"
            })
        ]
    ]
    
    for cmd in commands:
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                print(f"âœ… Connection {cmd[3]} created successfully")
            else:
                print(f"âŒ Failed to create connection {cmd[3]}: {result.stderr}")
        except Exception as e:
            print(f"âŒ Error creating connection: {e}")
```

### Airflow 3 Best Practices Implementation

#### 1. Modern DAG Patterns
```python
# Airflow 3 best practices for DAG definition
from airflow import DAG
from airflow.decorators import dag, task
from airflow.models.param import Param
from datetime import datetime, timedelta

# Method 1: Traditional DAG with Airflow 3 enhancements
@dag(
    dag_id="modern_etl_pipeline_v3",
    description="Modern ETL pipeline using Airflow 3 best practices",
    
    # Airflow 3 enhanced scheduling
    schedule="@daily",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    max_active_runs=1,
    
    # Airflow 3 parameters with validation
    params={
        "environment": Param(
            default="production",
            enum=["development", "staging", "production"],
            description="Target environment"
        ),
        "full_refresh": Param(
            default=False,
            type="boolean", 
            description="Perform full data refresh"
        )
    },
    
    # Airflow 3 enhanced tags and documentation
    tags=["etl", "finance", "airflow-3", "production"],
    doc_md="""
    # Modern ETL Pipeline
    
    This DAG demonstrates Airflow 3 best practices including:
    - TaskFlow API usage
    - Enhanced parameter validation
    - Improved error handling
    - Dynamic task generation
    """,
    
    # Airflow 3 default args with new features
    default_args={
        "owner": "data-engineering",
        "depends_on_past": False,
        "email_on_failure": False,
        "email_on_retry": False,
        "retries": 2,
        "retry_delay": timedelta(minutes=5),
        "retry_exponential_backoff": True,  # New in Airflow 3
        "max_retry_delay": timedelta(minutes=30),
    }
)
def modern_etl_dag_v3():
    """
    Modern ETL DAG using Airflow 3 TaskFlow API.
    """
    
    @task(task_id="validate_environment")
    def validate_environment_config(**context) -> dict:
        """Validate environment configuration."""
        params = context["params"]
        
        validation_results = {
            "environment": params["environment"],
            "full_refresh": params["full_refresh"],
            "validation_passed": True,
            "warnings": []
        }
        
        # Environment-specific validations
        if params["environment"] == "production" and params["full_refresh"]:
            validation_results["warnings"].append(
                "Full refresh in production - ensure data backup is available"
            )
        
        return validation_results
    
    @task(task_id="extract_data")
    def extract_data(validation_result: dict) -> dict:
        """Extract data based on environment configuration."""
        env = validation_result["environment"]
        full_refresh = validation_result["full_refresh"]
        
        # Environment-specific extraction logic
        extraction_config = {
            "development": {"days_back": 7, "batch_size": 100},
            "staging": {"days_back": 30, "batch_size": 500},
            "production": {"days_back": 365 if full_refresh else 7, "batch_size": 1000}
        }
        
        config = extraction_config[env]
        
        # Simulate data extraction
        extracted_records = config["batch_size"] * (config["days_back"] // 7)
        
        return {
            "environment": env,
            "records_extracted": extracted_records,
            "configuration": config
        }
    
    @task(task_id="transform_data")
    def transform_data(extraction_result: dict) -> dict:
        """Transform extracted data."""
        records = extraction_result["records_extracted"]
        
        # Simulate transformation
        transformed_records = int(records * 0.95)  # Assume 5% data loss
        
        return {
            "input_records": records,
            "output_records": transformed_records,
            "transformation_rate": transformed_records / records if records > 0 else 0
        }
    
    @task(task_id="load_data")
    def load_data(transformation_result: dict) -> dict:
        """Load transformed data."""
        records = transformation_result["output_records"]
        
        # Simulate loading
        loaded_records = records  # Assume successful load
        
        return {
            "records_loaded": loaded_records,
            "load_timestamp": datetime.now().isoformat()
        }
    
    # Define task dependencies using TaskFlow API
    validation = validate_environment_config()
    extraction = extract_data(validation)
    transformation = transform_data(extraction)
    loading = load_data(transformation)
    
    # Return the final task for potential downstream dependencies
    return loading

# Instantiate the DAG
etl_dag_v3 = modern_etl_dag_v3()
```

#### 2. Airflow 3 Error Handling Patterns
```python
# Advanced error handling with Airflow 3
from airflow.exceptions import AirflowSkipException, AirflowFailException
from airflow.providers.postgres.hooks.postgres import PostgresHook

@task(task_id="robust_data_processing", retries=3)
def robust_data_processing(**context) -> dict:
    """
    Demonstrate robust error handling in Airflow 3.
    """
    
    try:
        # Database operation with error handling
        hook = PostgresHook(postgres_conn_id="postgres_default")
        
        # Test connection
        connection = hook.get_conn()
        cursor = connection.cursor()
        
        # Critical data quality check
        cursor.execute("""
            SELECT COUNT(*) as record_count,
                   COUNT(DISTINCT symbol) as symbol_count
            FROM fact_stock_prices 
            WHERE date_value = CURRENT_DATE
        """)
        
        result = cursor.fetchone()
        record_count, symbol_count = result
        
        # Business rule validation
        if record_count == 0:
            # No data for today - check if it's a holiday
            cursor.execute("""
                SELECT is_trading_day 
                FROM dim_dates 
                WHERE date_value = CURRENT_DATE
            """)
            
            trading_day_result = cursor.fetchone()
            if trading_day_result and not trading_day_result[0]:
                # It's not a trading day - skip gracefully
                raise AirflowSkipException("Skipping processing - not a trading day")
            else:
                # It's a trading day but no data - this is an error
                raise AirflowFailException("No data available for trading day")
        
        # Data quality thresholds
        if symbol_count < 7:  # Magnificent Seven stocks
            raise AirflowFailException(
                f"Insufficient symbol coverage: {symbol_count} < 7"
            )
        
        # Success path
        cursor.close()
        connection.close()
        
        return {
            "record_count": record_count,
            "symbol_count": symbol_count,
            "status": "success",
            "processing_date": context["logical_date"].isoformat()
        }
        
    except (AirflowSkipException, AirflowFailException):
        # Re-raise Airflow exceptions
        raise
        
    except Exception as e:
        # Log and convert unexpected exceptions
        context["task_instance"].log.error(f"Unexpected error: {str(e)}")
        
        # Determine if this should be a skip or fail based on error type
        if "connection" in str(e).lower():
            # Connection issues - might be temporary
            raise AirflowFailException(f"Database connection error: {str(e)}")
        else:
            # Other errors - log and re-raise for retry
            raise
```

### Airflow 3 Testing Patterns

```python
# Airflow 3 testing best practices
import pytest
from airflow.models import DagBag
from airflow.utils.dag_cycle import check_cycle
from datetime import datetime

class TestAirflow3DAGs:
    """Test suite for Airflow 3 DAGs."""
    
    def test_dag_loading(self):
        """Test that all DAGs load without errors in Airflow 3."""
        dag_bag = DagBag(dag_folder="dags/", include_examples=False)
        
        # Check for import errors
        assert len(dag_bag.import_errors) == 0, f"DAG import errors: {dag_bag.import_errors}"
        
        # Ensure we have DAGs
        assert len(dag_bag.dags) > 0, "No DAGs found"
        
        # Airflow 3 specific validations
        for dag_id, dag in dag_bag.dags.items():
            # Check for cycles
            check_cycle(dag)
            
            # Validate Airflow 3 features
            assert hasattr(dag, 'doc_md') or dag.doc_md is None
            assert hasattr(dag, 'params')
            
            # Check modern scheduling
            if hasattr(dag, 'schedule'):
                assert dag.schedule is not None
    
    def test_task_dependencies(self):
        """Test task dependencies are properly defined."""
        dag_bag = DagBag(dag_folder="dags/", include_examples=False)
        
        for dag_id, dag in dag_bag.dags.items():
            # Check for orphaned tasks
            for task in dag.tasks:
                # Every task should have either upstream or downstream dependencies
                # unless it's explicitly a start/end task
                if not task.upstream_task_ids and not task.downstream_task_ids:
                    if task.task_id not in ['start', 'end', 'start_pipeline', 'end_pipeline']:
                        pytest.fail(f"Orphaned task found: {task.task_id} in DAG {dag_id}")
    
    def test_airflow_3_features(self):
        """Test Airflow 3 specific features are properly used."""
        dag_bag = DagBag(dag_folder="dags/", include_examples=False)
        
        for dag_id, dag in dag_bag.dags.items():
            # Check for proper parameter usage
            if hasattr(dag, 'params') and dag.params:
                for param_name, param in dag.params.items():
                    assert hasattr(param, 'default'), f"Parameter {param_name} missing default"
            
            # Check for proper documentation
            if dag.doc_md:
                assert len(dag.doc_md.strip()) > 0, f"Empty documentation in DAG {dag_id}"
            
            # Check for modern scheduling
            if hasattr(dag, 'schedule'):
                # Ensure we're using the new schedule parameter, not schedule_interval
                assert not hasattr(dag, 'schedule_interval') or dag.schedule_interval is None
```

## Deployment and Testing

### Airflow 3 Deployment Strategy

#### Pre-deployment Validation
```bash
#!/bin/bash
# Airflow 3 deployment validation script

echo "ðŸ” Validating Airflow 3 deployment..."

# Check Airflow 3 installation
echo "âœ… Checking Airflow 3 installation..."
airflow version

# Validate configuration
echo "âœ… Validating Airflow 3 configuration..."
airflow config validate

# Check database connectivity
echo "âœ… Testing database connectivity..."
airflow db check

# Validate DAG integrity
echo "âœ… Validating DAG integrity..."
airflow dags list --output table

# Check for import errors
echo "âœ… Checking for DAG import errors..."
python -c "
from airflow.models import DagBag
import sys

dag_bag = DagBag(include_examples=False)
if dag_bag.import_errors:
    print('âŒ DAG import errors found:')
    for filename, error in dag_bag.import_errors.items():
        print(f'  {filename}: {error}')
    sys.exit(1)
else:
    print('âœ… No DAG import errors')
"

# Test connections
echo "âœ… Testing Airflow 3 connections..."
airflow connections test postgres_default

echo "ðŸŽ‰ Airflow 3 validation completed successfully!"
```

#### Deployment Steps for Airflow 3
```bash
# 1. Environment preparation
export AIRFLOW_HOME=/opt/airflow
export AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
export AIRFLOW__CORE__LOAD_EXAMPLES=False

# 2. Database schema deployment
echo "ðŸ“‹ Deploying database schema..."
psql $DATABASE_URL -f sql/ddl/001_create_dimensions.sql
psql $DATABASE_URL -f sql/ddl/002_create_facts.sql
psql $DATABASE_URL -f sql/ddl/003_create_views.sql
psql $DATABASE_URL -f sql/ddl/004_create_indexes.sql

# 3. Airflow 3 initialization
echo "ðŸš€ Initializing Airflow 3..."
airflow db init

# 4. Create admin user
echo "ðŸ‘¤ Creating admin user..."
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@ticker-converter.com \
    --password secure_admin_password

# 5. Deploy DAGs and SQL files
echo "ðŸ“‚ Deploying DAGs and SQL files..."
cp -r dags/ $AIRFLOW_HOME/dags/
cp -r airflow/sql/ $AIRFLOW_HOME/sql/
cp -r airflow/functions/ $AIRFLOW_HOME/functions/

# 6. Set up connections and variables
echo "ðŸ”— Setting up connections..."
airflow connections add postgres_default \
    --conn-type postgres \
    --conn-host localhost \
    --conn-schema ticker_converter \
    --conn-login ticker_user \
    --conn-password $POSTGRES_PASSWORD \
    --conn-port 5432

# 7. Import variables
echo "âš™ï¸ Importing variables..."
airflow variables import airflow/config/variables.json

# 8. Test DAG integrity
echo "ðŸ§ª Testing DAG integrity..."
airflow dags test ticker_converter_daily_etl_v3 2024-01-01

# 9. Enable DAGs
echo "â–¶ï¸ Enabling DAGs..."
airflow dags unpause ticker_converter_daily_etl_v3
airflow dags unpause test_etl_dag

echo "âœ… Airflow 3 deployment completed successfully!"
```

### Airflow 3 Testing Framework

#### Unit Testing for Airflow 3 DAGs
```python
# tests/test_airflow_3_dags.py
import pytest
from airflow.models import DagBag, TaskInstance
from airflow.utils.state import State
from airflow.utils.types import DagRunType
from datetime import datetime, timedelta

class TestAirflow3DAGs:
    """Comprehensive testing for Airflow 3 DAGs."""
    
    @pytest.fixture
    def dag_bag(self):
        """Load DAGs for testing."""
        return DagBag(dag_folder="dags/", include_examples=False)
    
    def test_dag_loaded_successfully(self, dag_bag):
        """Test that all DAGs load without errors."""
        assert len(dag_bag.import_errors) == 0, f"Import errors: {dag_bag.import_errors}"
        assert "ticker_converter_daily_etl_v3" in dag_bag.dags
    
    def test_dag_structure(self, dag_bag):
        """Test DAG structure and properties."""
        dag = dag_bag.get_dag("ticker_converter_daily_etl_v3")
        
        # Check DAG properties
        assert dag.schedule == "@daily"
        assert dag.catchup is False
        assert dag.max_active_runs == 1
        
        # Check required tasks exist
        required_tasks = [
            "validate_dag_parameters",
            "dimension_loading_v3",
            "data_extraction_v3", 
            "transformation_and_quality_v3"
        ]
        
        task_ids = [task.task_id for task in dag.tasks]
        for required_task in required_tasks:
            assert any(required_task in task_id for task_id in task_ids), \
                f"Required task {required_task} not found"
    
    def test_task_dependencies(self, dag_bag):
        """Test that task dependencies are correctly defined."""
        dag = dag_bag.get_dag("ticker_converter_daily_etl_v3")
        
        # Test that no tasks are orphaned
        for task in dag.tasks:
            if task.task_id not in ["start_etl_pipeline", "end_etl_pipeline"]:
                assert (task.upstream_task_ids or task.downstream_task_ids), \
                    f"Task {task.task_id} is orphaned"
    
    def test_airflow_3_features(self, dag_bag):
        """Test Airflow 3 specific features."""
        dag = dag_bag.get_dag("ticker_converter_daily_etl_v3")
        
        # Check parameters
        assert hasattr(dag, 'params')
        assert "environment" in dag.params
        assert "full_refresh" in dag.params
        
        # Check documentation
        assert dag.doc_md is not None
        assert len(dag.doc_md.strip()) > 0
        
        # Check tags
        assert "airflow-3" in dag.tags

# Integration testing
@pytest.mark.integration
class TestAirflow3Integration:
    """Integration tests for Airflow 3 functionality."""
    
    def test_database_connection(self):
        """Test database connectivity through Airflow 3."""
        from airflow.providers.postgres.hooks.postgres import PostgresHook
        
        hook = PostgresHook(postgres_conn_id="postgres_default")
        
        # Test connection
        connection = hook.get_conn()
        cursor = connection.cursor()
        
        # Test basic query
        cursor.execute("SELECT 1")
        result = cursor.fetchone()
        
        assert result[0] == 1
        
        cursor.close()
        connection.close()
    
    def test_sql_file_accessibility(self):
        """Test that SQL files are accessible from DAG context."""
        import os
        from pathlib import Path
        
        # Check SQL file structure
        sql_path = Path(os.getenv("AIRFLOW_HOME", ".")) / "sql" / "etl"
        
        required_sql_files = [
            "load_stock_dimension.sql",
            "load_date_dimension.sql", 
            "load_currency_dimension.sql",
            "daily_transforms.sql",
            "data_quality_checks.sql",
            "cleanup_old_data.sql"
        ]
        
        for sql_file in required_sql_files:
            file_path = sql_path / sql_file
            assert file_path.exists(), f"SQL file {sql_file} not found at {file_path}"
            assert file_path.stat().st_size > 0, f"SQL file {sql_file} is empty"
```

#### Performance Testing
```python
# tests/test_airflow_3_performance.py
import pytest
import time
from airflow.models import DagBag
from airflow.utils.dag_cycle import check_cycle

class TestAirflow3Performance:
    """Performance testing for Airflow 3 DAGs."""
    
    def test_dag_loading_performance(self):
        """Test that DAGs load within acceptable time limits."""
        start_time = time.time()
        
        dag_bag = DagBag(dag_folder="dags/", include_examples=False)
        
        load_time = time.time() - start_time
        
        # Should load all DAGs within 30 seconds
        assert load_time < 30, f"DAG loading took {load_time:.2f} seconds"
        
        # Should have loaded DAGs without errors
        assert len(dag_bag.import_errors) == 0
    
    def test_task_execution_time(self):
        """Test task execution performance."""
        from airflow.providers.postgres.hooks.postgres import PostgresHook
        
        hook = PostgresHook(postgres_conn_id="postgres_default")
        
        # Test simple query performance
        start_time = time.time()
        
        result = hook.get_first("SELECT COUNT(*) FROM dim_stocks")
        
        query_time = time.time() - start_time
        
        # Database queries should complete within 5 seconds
        assert query_time < 5, f"Query took {query_time:.2f} seconds"
        assert result is not None
```

### Monitoring and Maintenance

#### Airflow 3 Health Monitoring
```python
# monitoring/airflow_3_health_check.py
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta
import logging

class Airflow3HealthMonitor:
    """Health monitoring for Airflow 3 deployment."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def check_dag_health(self) -> dict:
        """Check overall DAG health."""
        hook = PostgresHook(postgres_conn_id="postgres_default")
        
        # Check recent DAG runs
        health_status = {
            "timestamp": datetime.now().isoformat(),
            "checks": {}
        }
        
        try:
            # Check for failed DAG runs in last 24 hours
            failed_runs = hook.get_first("""
                SELECT COUNT(*) FROM dag_run 
                WHERE state = 'failed' 
                AND execution_date > NOW() - INTERVAL '24 hours'
            """)
            
            health_status["checks"]["failed_runs_24h"] = {
                "count": failed_runs[0] if failed_runs else 0,
                "status": "healthy" if (failed_runs[0] if failed_runs else 0) == 0 else "warning"
            }
            
            # Check for stuck tasks
            stuck_tasks = hook.get_first("""
                SELECT COUNT(*) FROM task_instance 
                WHERE state = 'running' 
                AND start_date < NOW() - INTERVAL '2 hours'
            """)
            
            health_status["checks"]["stuck_tasks"] = {
                "count": stuck_tasks[0] if stuck_tasks else 0,
                "status": "healthy" if (stuck_tasks[0] if stuck_tasks else 0) == 0 else "critical"
            }
            
            # Check data freshness
            latest_data = hook.get_first("""
                SELECT MAX(d.date) 
                FROM fact_stock_prices p
                JOIN dim_dates d ON p.date_id = d.date_id
            """)
            
            if latest_data and latest_data[0]:
                days_old = (datetime.now().date() - latest_data[0]).days
                health_status["checks"]["data_freshness"] = {
                    "latest_date": latest_data[0].isoformat(),
                    "days_old": days_old,
                    "status": "healthy" if days_old <= 1 else "warning" if days_old <= 3 else "critical"
                }
            
        except Exception as e:
            health_status["error"] = str(e)
            self.logger.error(f"Health check failed: {e}")
        
        return health_status
    
    def send_health_report(self, health_status: dict):
        """Send health report to monitoring system."""
        # Implementation would integrate with monitoring system
        # (e.g., Prometheus, DataDog, CloudWatch)
        
        critical_issues = [
            check for check in health_status.get("checks", {}).values() 
            if check.get("status") == "critical"
        ]
        
        if critical_issues:
            self.logger.critical(f"Critical health issues detected: {critical_issues}")
            # Send alerts
        
        warnings = [
            check for check in health_status.get("checks", {}).values()
            if check.get("status") == "warning"
        ]
        
        if warnings:
            self.logger.warning(f"Health warnings detected: {warnings}")

# Usage in DAG
@task(task_id="health_monitoring")
def run_health_check(**context):
    """Run health check as part of DAG."""
    monitor = Airflow3HealthMonitor()
    health_status = monitor.check_dag_health()
    monitor.send_health_report(health_status)
    
    return health_status
```

## Integration with Project Documentation

This guide integrates architectural patterns from the project's `my_docs/airflow_dag_documentation.md`, specifically:

### âœ… **SQL-Centric Architecture Implementation**:
- **Task Structure**: Adopted the exact linear pipeline: `fetch_stock_data â†’ load_dimensions â†’ transform_to_warehouse â†’ data_quality_check â†’ cleanup_staging`
- **SQL File Organization**: Integrated external SQL file patterns (`sql/etl/*.sql`) for version control and maintainability
- **Business Logic Separation**: Python tasks limited to API ingestion only, all transformations in PostgreSQL
- **Data Quality Patterns**: Implemented Magnificent Seven stock validation, OHLC consistency checks, and currency rate completeness validation

### âœ… **Operational Patterns**:
- **Idempotent Operations**: All SQL tasks use `ON CONFLICT DO UPDATE` for safe re-execution
- **Error Handling**: PostgreSQL stored procedures with `RAISE EXCEPTION` for data quality failures
- **Retention Management**: SQL-based cleanup with 7-day staging retention policy
- **Connection Management**: PostgreSQL connection configuration with enhanced Airflow 3 parameters

### âœ… **Schedule and Monitoring**:
- **Market-Aware Scheduling**: Updated to 6 PM EST weekdays for post-market data processing
- **Data Freshness Monitoring**: Business day vs. trading day validation logic
- **Performance Optimization**: PostgreSQL indexes on staging tables for improved ETL performance

This ensures the Airflow 3 implementation aligns with the project's established SQL-first architecture while incorporating modern Airflow features and best practices.

---

## Summary

This comprehensive Airflow 3 setup guide provides:

### âœ… **Key Features Covered**:
1. **Modern Installation**: Airflow 3.0+ with updated providers
2. **Enhanced Configuration**: New security features and performance optimizations
3. **TaskFlow API**: Modern Python-based task definitions
4. **Task Groups**: Logical organization of related tasks
5. **Parameter Validation**: Built-in parameter validation and typing
6. **Error Handling**: Sophisticated error handling with skip/fail logic
7. **Testing Framework**: Comprehensive unit and integration testing
8. **Performance Monitoring**: Health checks and performance validation
9. **Deployment Strategy**: Production-ready deployment procedures

### ðŸ”§ **Airflow 3 Enhancements**:
- Enhanced scheduler performance with parallel parsing
- Improved security with secret key management
- Better connection pooling and database performance
- Modern TaskFlow API with improved typing
- Enhanced parameter validation and documentation
- Improved error handling and monitoring capabilities

### ðŸ“š **Best Practices Implemented**:
- SQL-centric approach with minimal Python business logic
- Proper separation of concerns between orchestration and business logic
- Comprehensive testing and validation procedures
- Production-ready error handling and monitoring
- Modern deployment and maintenance strategies

This guide ensures your Airflow 3 implementation follows current best practices while maintaining the architectural principles of the ticker-converter project.
