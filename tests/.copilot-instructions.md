# Testing Framework Instructions

This directory contains all testing infrastructure for the ticker-converter application. Follow these instructions for comprehensive, maintainable test development.

## Directory Purpose

The `/tests/` directory provides complete test coverage including:
- **Unit tests** for individual functions and classes
- **Integration tests** for database and API interactions
- **Quality assurance** tools and validation scripts
- **Test fixtures** and mock objects for reliable testing

## Test Organization

### Directory Structure
```
tests/
├── conftest.py                    # Pytest configuration and shared fixtures
├── pytest.ini                    # Pytest settings and configuration
├── unit/                         # Unit tests mirroring src/ structure
│   ├── api/                      # API endpoint tests
│   ├── api_clients/              # External API client tests
│   ├── data_ingestion/           # Data processing tests
│   ├── data_models/              # Model validation tests
│   └── sql/                      # SQL query tests
├── integration/                  # Integration and end-to-end tests
│   ├── test_api_integration.py   # API workflow tests
│   └── test_database_operations.py
├── quality/                      # Code quality validation
│   ├── quality_check.py          # Quality assessment tools
│   └── run_mypy.py              # Type checking validation
└── fixtures/                     # Test data and mock objects
    ├── api_responses/            # Mock API response data
    ├── database/                 # Test database schemas
    └── sample_data/              # Sample datasets
```

## Testing Standards

### Test Structure Requirements

**Every test module must**:
- **Mirror source structure**: `src/module.py` → `tests/unit/test_module.py`
- **Import modules consistently**: Use absolute imports from src
- **Follow naming conventions**: Test functions start with `test_`
- **Include comprehensive docstrings**: Explain test purpose and scenarios

### Test Function Template
```python
import pytest
from unittest.mock import Mock, patch
from src.ticker_converter.data_ingestion.nyse_fetcher import NYSEFetcher

class TestNYSEFetcher:
    """Test suite for NYSE data fetching functionality."""
    
    def test_fetch_valid_symbol_success(self, mock_api_response):
        """Test successful data fetching for valid stock symbol.
        
        Verifies that:
        - Valid symbol returns expected data structure
        - API response is properly parsed
        - Data validation passes
        """
        # Arrange
        fetcher = NYSEFetcher(api_key="test_key")
        symbol = "AAPL"
        
        # Act
        result = fetcher.fetch_stock_data(symbol)
        
        # Assert
        assert result is not None
        assert result.symbol == symbol
        assert len(result.data_points) > 0
        
    def test_fetch_invalid_symbol_raises_error(self):
        """Test error handling for invalid stock symbol.
        
        Verifies that:
        - Invalid symbol raises ValidationError
        - Error message is descriptive
        - No partial data is returned
        """
        # Arrange
        fetcher = NYSEFetcher(api_key="test_key")
        invalid_symbol = "INVALID123"
        
        # Act & Assert
        with pytest.raises(ValidationError) as exc_info:
            fetcher.fetch_stock_data(invalid_symbol)
        
        assert "invalid symbol" in str(exc_info.value).lower()
```

## Unit Testing Guidelines

### Test Coverage Requirements
- **Minimum 50% overall coverage** (currently at 54.27%)
- **100% coverage for new functionality**
- **Critical path coverage** for data processing and API operations
- **Edge case coverage** for error conditions

### Mocking Strategies

**External Dependencies**:
```python
@patch('src.ticker_converter.api_clients.client.requests.get')
def test_api_call_with_timeout(self, mock_get):
    """Test API client timeout handling."""
    mock_get.side_effect = requests.Timeout("Connection timeout")
    
    client = APIClient()
    with pytest.raises(APITimeoutError):
        client.fetch_data("AAPL")
```

**Database Operations**:
```python
@pytest.fixture
def mock_database_session():
    """Provide mock database session for testing."""
    session = Mock()
    session.query.return_value.filter.return_value.first.return_value = None
    return session

def test_database_insert(mock_database_session):
    """Test database record insertion."""
    manager = DatabaseManager(session=mock_database_session)
    result = manager.insert_stock_data(sample_data)
    
    mock_database_session.add.assert_called_once()
    mock_database_session.commit.assert_called_once()
```

### Parameterized Testing
```python
@pytest.mark.parametrize("symbol,expected_valid", [
    ("AAPL", True),
    ("GOOGL", True),
    ("MSFT", True),
    ("", False),
    ("123", False),
    ("TOOLONG", False),
])
def test_symbol_validation(symbol, expected_valid):
    """Test stock symbol validation logic."""
    validator = SymbolValidator()
    assert validator.is_valid(symbol) == expected_valid
```

## Integration Testing

### Database Integration Tests
```python
@pytest.mark.integration
class TestDatabaseIntegration:
    """Integration tests for database operations."""
    
    @pytest.fixture(autouse=True)
    def setup_test_database(self):
        """Set up clean test database for each test."""
        # Create test database
        # Run migrations
        # Clean up after test
        
    def test_full_etl_pipeline(self):
        """Test complete ETL pipeline with real database."""
        # Test data ingestion → transformation → loading
        pass
```

### API Integration Tests
```python
@pytest.mark.integration
@pytest.mark.slow
class TestAPIIntegration:
    """Integration tests for external API interactions."""
    
    def test_alpha_vantage_integration(self):
        """Test real Alpha Vantage API integration."""
        # Only run with valid API key
        if not os.getenv('ALPHA_VANTAGE_API_KEY'):
            pytest.skip("API key not available")
            
        # Test real API call with rate limiting
```

## Test Fixtures and Data

### Fixture Organization
```python
# conftest.py - Shared fixtures
@pytest.fixture
def sample_stock_data():
    """Provide sample stock data for testing."""
    return {
        'symbol': 'AAPL',
        'date': '2023-01-01',
        'open': 150.00,
        'high': 155.00,
        'low': 149.00,
        'close': 154.00,
        'volume': 1000000
    }

@pytest.fixture
def mock_api_response():
    """Provide mock API response data."""
    with open('tests/fixtures/api_responses/alpha_vantage_response.json') as f:
        return json.load(f)
```

### Test Data Management
- **No hardcoded data** in test functions
- **Use fixtures** for reusable test data
- **Store sample data** in `/tests/fixtures/`
- **Version control** sample data files
- **Document data structure** in fixture docstrings

## Quality Assurance Testing

### Code Quality Validation
```python
# tests/quality/quality_check.py
def test_pylint_score():
    """Ensure pylint score remains at 10.00/10."""
    result = subprocess.run(['pylint', 'src/'], capture_output=True, text=True)
    assert "10.00/10" in result.stdout

def test_mypy_validation():
    """Ensure MyPy type checking passes."""
    result = subprocess.run(['mypy', 'src/'], capture_output=True, text=True)
    assert result.returncode == 0

def test_black_formatting():
    """Ensure code follows Black formatting."""
    result = subprocess.run(['black', '--check', 'src/'], capture_output=True)
    assert result.returncode == 0
```

### Test Coverage Validation
```python
def test_coverage_threshold():
    """Ensure test coverage meets minimum threshold."""
    result = subprocess.run([
        'pytest', '--cov=src', '--cov-report=term-missing'
    ], capture_output=True, text=True)
    
    # Extract coverage percentage
    coverage_line = [line for line in result.stdout.split('\n') 
                    if 'TOTAL' in line][-1]
    coverage_percent = int(coverage_line.split()[-1].rstrip('%'))
    
    assert coverage_percent >= 50
```

## Performance Testing

### Performance Benchmarks
```python
@pytest.mark.performance
def test_data_processing_performance():
    """Benchmark data processing performance."""
    import time
    
    start_time = time.time()
    processor = DataProcessor()
    processor.process_large_dataset(sample_large_data)
    execution_time = time.time() - start_time
    
    # Should process 10k records in under 5 seconds
    assert execution_time < 5.0
```

### Memory Usage Testing
```python
@pytest.mark.memory
def test_memory_usage_limits():
    """Ensure memory usage stays within limits."""
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss
    
    # Perform memory-intensive operation
    processor = DataProcessor()
    processor.process_large_dataset(sample_data)
    
    final_memory = process.memory_info().rss
    memory_increase = final_memory - initial_memory
    
    # Should not increase memory by more than 100MB
    assert memory_increase < 100 * 1024 * 1024
```

## Test Configuration

### Pytest Configuration (pytest.ini)
```ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --cov=src
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-fail-under=50

markers =
    integration: marks tests as integration tests (deselect with '-m "not integration"')
    slow: marks tests as slow (deselect with '-m "not slow"')
    performance: marks tests as performance benchmarks
    memory: marks tests as memory usage tests
```

### Environment-Specific Testing
```python
# Different configurations for different environments
@pytest.fixture
def database_url():
    """Provide database URL for current environment."""
    if os.getenv('TESTING_ENV') == 'ci':
        return 'sqlite:///:memory:'
    elif os.getenv('TESTING_ENV') == 'local':
        return 'sqlite:///test.db'
    else:
        return 'postgresql://test:test@localhost/test_db'
```

## Error Simulation Testing

### API Failure Simulation
```python
def test_api_failure_handling():
    """Test handling of various API failure scenarios."""
    scenarios = [
        (requests.ConnectionError, "Network connection failed"),
        (requests.Timeout, "Request timeout"),
        (requests.HTTPError, "HTTP error response"),
        (ValueError, "Invalid response format")
    ]
    
    for exception, description in scenarios:
        with patch('requests.get') as mock_get:
            mock_get.side_effect = exception(description)
            
            client = APIClient()
            with pytest.raises(APIError):
                client.fetch_data("AAPL")
```

### Database Failure Simulation
```python
def test_database_failure_recovery():
    """Test recovery from database connection failures."""
    with patch('sqlalchemy.create_engine') as mock_engine:
        mock_engine.side_effect = OperationalError("Connection failed", None, None)
        
        manager = DatabaseManager()
        with pytest.raises(DatabaseConnectionError):
            manager.connect()
```

## Test Execution Guidelines

### Local Development
```bash
# Run all tests
pytest

# Run specific test categories
pytest -m "not integration"  # Skip integration tests
pytest -m "not slow"         # Skip slow tests
pytest tests/unit/           # Only unit tests

# Run with coverage
pytest --cov=src tests/

# Run specific test file
pytest tests/unit/data_ingestion/test_nyse_fetcher.py
```

### CI/CD Integration
```bash
# Full test suite for CI/CD
pytest --cov=src --cov-fail-under=50 tests/

# Quality validation
python tests/quality/quality_check.py
python tests/quality/run_mypy.py
```

## Debugging Tests

### Test Debugging Strategies
```python
# Use pytest debugging features
def test_complex_operation():
    """Test with debugging support."""
    # Set breakpoint for debugging
    import pdb; pdb.set_trace()
    
    # Or use pytest's built-in debugging
    pytest.set_trace()
    
    result = complex_operation()
    assert result.is_valid
```

### Verbose Output
```bash
# Run tests with maximum verbosity
pytest -vv tests/

# Show all output (including print statements)
pytest -s tests/

# Show detailed failure information
pytest --tb=long tests/
```

## Maintenance Guidelines

### Regular Test Maintenance
- **Update fixtures** when data models change
- **Refresh mock responses** when API contracts change
- **Remove obsolete tests** when functionality is removed
- **Add tests** for new functionality immediately

### Test Performance Optimization
- **Use appropriate fixtures** scope (function, class, module, session)
- **Mock expensive operations** consistently
- **Parallelize tests** where possible
- **Profile test execution** to identify bottlenecks

### Documentation Updates
- **Keep test documentation** current with implementation
- **Document test scenarios** and edge cases
- **Explain complex test setups** thoroughly
- **Maintain fixture documentation** for reusability
