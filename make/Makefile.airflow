# ============================================================================
# AIRFLOW ORCHESTRATION AND MANAGEMENT
# ============================================================================
# This module handles Apache Airflow operations, including service management,
# DAG operations, and workflow orchestration
#
# Dependencies: Makefile.platform, Makefile.env, Makefile.database
# ============================================================================

# Airflow configuration
AIRFLOW_HOME := $(shell pwd)/airflow
AIRFLOW_DAGS_FOLDER := $(shell pwd)/dags

# ============================================================================
# AIRFLOW SERVICE MANAGEMENT
# ============================================================================

.PHONY: airflow-start airflow-stop airflow-restart airflow-status _check_airflow_deps

airflow-start: _load_env _check_airflow_deps airflow-init ## üåä Start Airflow (scheduler + webserver)
	@printf "\033[0;34mStarting Apache Airflow...\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__CORE__LOAD_EXAMPLES="false" && \
	export AIRFLOW__CORE__AUTH_MANAGER="airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	export AIRFLOW__API_AUTH__JWT_SECRET="$$AIRFLOW__API_AUTH__JWT_SECRET" && \
	echo -e "\033[0;33mStarting Airflow scheduler...\033[0m\n" && \
	nohup $(PYTHON) -m airflow scheduler > airflow/airflow-scheduler.out 2> airflow/airflow-scheduler.err & \
	echo $$! > airflow/scheduler.pid && \
	echo -e "\033[0;33mStarting Airflow API server...\033[0m\n" && \
	nohup $(PYTHON) -m airflow api-server --port 8080 > airflow/airflow-webserver.out 2> airflow/airflow-webserver.err & \
	echo $$! > airflow/webserver.pid && \
	sleep 3 && \
	echo -e "\033[0;32m‚úì Airflow started\033[0m\n" && \
	echo -e "\033[0;36mAPI Server: http://localhost:8080\033[0m\n" && \
	echo -e "\033[0;36mLogs: airflow/airflow-*.out\033[0m\n"

airflow-stop: ## üåä Stop all Airflow processes
	@printf "\033[0;34mStopping Apache Airflow...\033[0m\n"
	@if [ -f airflow/scheduler.pid ]; then \
		echo -e "\033[0;33mStopping Airflow scheduler...\033[0m\n"; \
		kill $$(cat airflow/scheduler.pid) 2>/dev/null || true; \
		rm -f airflow/scheduler.pid; \
	fi
	@if [ -f airflow/webserver.pid ]; then \
		echo -e "\033[0;33mStopping Airflow webserver...\033[0m\n"; \
		kill $$(cat airflow/webserver.pid) 2>/dev/null || true; \
		rm -f airflow/webserver.pid; \
	fi
	@pkill -f "airflow scheduler" 2>/dev/null || true
	@pkill -f "airflow webserver" 2>/dev/null || true
	@printf "\033[0;32m‚úì Airflow stopped\033[0m\n"

airflow-restart: airflow-stop airflow-start ## üåä Restart Airflow services
	@printf "\033[0;32m‚úì Airflow restarted\033[0m\n"

airflow-status: ## üåä Validate Airflow process status
	@printf "\033[0;36mAirflow process status:\033[0m\n"
	@if [ -f airflow/scheduler.pid ] && kill -0 $$(cat airflow/scheduler.pid) 2>/dev/null; then \
		echo -e "\033[0;32m‚úì Scheduler running (PID: $$(cat airflow/scheduler.pid))\033[0m\n"; \
	else \
		echo -e "\033[0;33m‚ö† Scheduler not running\033[0m\n"; \
	fi
	@if [ -f airflow/webserver.pid ] && kill -0 $$(cat airflow/webserver.pid) 2>/dev/null; then \
		echo -e "\033[0;32m‚úì Webserver running (PID: $$(cat airflow/webserver.pid))\033[0m\n"; \
	else \
		echo -e "\033[0;33m‚ö† Webserver not running\033[0m\n"; \
	fi
	@if pgrep -f "airflow" >/dev/null; then \
		echo -e "\033[0;32mActive Airflow processes:\033[0m\n"; \
		pgrep -f "airflow" -l; \
	else \
		echo -e "\033[0;33m‚ö† No Airflow processes found\033[0m\n"; \
	fi

# ============================================================================
# AIRFLOW INITIALIZATION AND CONFIGURATION
# ============================================================================

.PHONY: airflow-init airflow-reset airflow-config _setup_airflow_dirs _init_airflow_db

airflow-init: _load_env _setup_airflow_dirs _init_airflow_db ## üåä Initialize Airflow database and admin user
	@printf "\033[0;34mInitializing Apache Airflow...\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__CORE__LOAD_EXAMPLES="false" && \
	export AIRFLOW__CORE__AUTH_MANAGER="airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	export AIRFLOW__API_AUTH__JWT_SECRET="$$AIRFLOW__API_AUTH__JWT_SECRET" && \
	echo -e "\033[0;33mCreating Airflow admin user...\033[0m\n" && \
	$(PYTHON) -m airflow users create \
		--username "$$AIRFLOW_ADMIN_USERNAME" \
		--firstname "$$AIRFLOW_ADMIN_FIRSTNAME" \
		--lastname "$$AIRFLOW_ADMIN_LASTNAME" \
		--role Admin \
		--email "$$AIRFLOW_ADMIN_EMAIL" \
		--password "$$AIRFLOW_ADMIN_PASSWORD" \
		2>/dev/null || echo -e "\033[0;33m‚ö† Admin user already exists or creation failed\033[0m\n" && \
	echo -e "\033[0;32m‚úì Airflow initialization completed\033[0m\n"

airflow-reset: _load_env ## üåä Reset Airflow configuration and database (DESTRUCTIVE)
	@printf "\033[0;31mWARNING: This will reset all Airflow data!\033[0m\n"
	@printf "\033[0;33mPress Ctrl+C to cancel, or Enter to continue...\033[0m\n"
	@read -p ""
	@$(MAKE) airflow-stop
	@printf "\033[0;33mRemoving Airflow database and logs...\033[0m\n"
	@rm -rf airflow/airflow.db airflow/logs/* airflow/airflow-*.out airflow/airflow-*.err
	@$(MAKE) airflow-init
	@printf "\033[0;32m‚úì Airflow reset completed\033[0m\n"

airflow-config: _load_env ## üåä Display Airflow configuration
	@printf "\033[0;36mAirflow Configuration:\033[0m\n"
	@set -a && . ./.env && set +a && \
	echo -e "\033[0;33mEnvironment Variables:\033[0m\n" && \
	echo -e "  AIRFLOW_HOME: $(AIRFLOW_HOME)" && \
	echo -e "  AIRFLOW__CORE__DAGS_FOLDER: $(AIRFLOW_DAGS_FOLDER)" && \
	echo -e "  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: $$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	echo -e "  AIRFLOW_ADMIN_USERNAME: $$AIRFLOW_ADMIN_USERNAME" && \
	echo -e "\033[0;33mAdmin Configuration:\033[0m\n" && \
	echo -e "  Username: $$AIRFLOW_ADMIN_USERNAME" && \
	echo -e "  Email: $$AIRFLOW_ADMIN_EMAIL" && \
	echo -e "  First Name: $$AIRFLOW_ADMIN_FIRSTNAME" && \
	echo -e "  Last Name: $$AIRFLOW_ADMIN_LASTNAME"

_setup_airflow_dirs: ## Internal: Create Airflow directory structure
	@printf "\033[0;33mSetting up Airflow directories...\033[0m\n"
	@mkdir -p airflow/logs airflow/plugins
	@printf "\033[0;32m‚úì Airflow directories created\033[0m\n"

_init_airflow_db: _load_env ## Internal: Initialize Airflow database
	@printf "\033[0;33mInitializing Airflow database...\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__CORE__LOAD_EXAMPLES="false" && \
	export AIRFLOW__CORE__AUTH_MANAGER="airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	export AIRFLOW__API_AUTH__JWT_SECRET="$$AIRFLOW__API_AUTH__JWT_SECRET" && \
	$(PYTHON) -m airflow db migrate

# ============================================================================
# DAG MANAGEMENT AND OPERATIONS
# ============================================================================

.PHONY: airflow-dag-list airflow-dag-test airflow-dag-trigger airflow-dag-info

airflow-dag-list: _load_env ## üåä List and display available DAGs
	@printf "\033[0;36mAvailable DAGs:\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow dags list

airflow-dag-test: _load_env ## üß™ Execute tests DAG execution (interactive)
	@printf "\033[0;36mDAG Testing\033[0m\n"
	@$(MAKE) airflow-dag-list
	@printf "\033[0;33mEnter DAG ID to test:\033[0m\n"
	@read -p "DAG ID: " dag_id && \
	echo -e "\033[0;33mEnter execution date (YYYY-MM-DD) or press Enter for today:\033[0m\n" && \
	read -p "Date: " exec_date && \
	exec_date=$${exec_date:-$$(date +%Y-%m-%d)} && \
	set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow dags test $$dag_id $$exec_date

airflow-dag-trigger: _load_env ## üåä Manually trigger DAG run (interactive)
	@printf "\033[0;36mDAG Triggering\033[0m\n"
	@$(MAKE) airflow-dag-list
	@printf "\033[0;33mEnter DAG ID to trigger:\033[0m\n"
	@read -p "DAG ID: " dag_id && \
	set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow dags trigger $$dag_id && \
	echo -e "\033[0;32m‚úì DAG $$dag_id triggered\033[0m\n"

airflow-dag-info: _load_env ## üåä Display DAG information (interactive)
	@printf "\033[0;36mDAG Information\033[0m\n"
	@$(MAKE) airflow-dag-list
	@printf "\033[0;33mEnter DAG ID for information:\033[0m\n"
	@read -p "DAG ID: " dag_id && \
	set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow dags show $$dag_id

# ============================================================================
# AIRFLOW UTILITIES AND DEBUGGING
# ============================================================================

.PHONY: airflow-shell airflow-logs airflow-debug airflow-webui

airflow-shell: _load_env ## üåä Open and display Airflow shell for debugging
	@printf "\033[0;36mOpening Airflow shell...\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -c "import airflow; from airflow import settings; print('Airflow shell ready. Access via settings object.'); import IPython; IPython.embed()" 2>/dev/null || \
	$(PYTHON) -c "import airflow; from airflow import settings; print('Airflow shell ready. Use settings object for configuration.')"

airflow-logs: ## üåä View Airflow logs
	@printf "\033[0;36mAirflow logs:\033[0m\n"
	@printf "\033[0;33mRecent scheduler logs:\033[0m\n"
	@tail -n 20 airflow/airflow-scheduler.out 2>/dev/null || echo -e "\033[0;33mNo scheduler logs found\033[0m\n"
	@printf "\033[0;33mRecent webserver logs:\033[0m\n"
	@tail -n 20 airflow/airflow-webserver.out 2>/dev/null || echo -e "\033[0;33mNo webserver logs found\033[0m\n"
	@printf "\033[0;33mRecent error logs:\033[0m\n"
	@tail -n 10 airflow/airflow-scheduler.err 2>/dev/null || echo -e "\033[0;33mNo scheduler errors\033[0m\n"
	@tail -n 10 airflow/airflow-webserver.err 2>/dev/null || echo -e "\033[0;33mNo webserver errors\033[0m\n"

airflow-debug: _load_env ## üåä Debug and troubleshoot Airflow configuration and setup
	@printf "\033[0;36mAirflow Debug Information:\033[0m\n"
	@set -a && . ./.env && set +a && \
	echo -e "\033[0;33mEnvironment Configuration:\033[0m\n" && \
	echo -e "  AIRFLOW_HOME: $(AIRFLOW_HOME)" && \
	echo -e "  AIRFLOW__CORE__DAGS_FOLDER: $(AIRFLOW_DAGS_FOLDER)" && \
	echo -e "  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: $$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	echo -e "\033[0;33mDirectory Structure:\033[0m\n" && \
	ls -la airflow/ 2>/dev/null || echo -e "  Airflow directory not found" && \
	echo -e "\033[0;33mDAGs Directory:\033[0m\n" && \
	ls -la dags/ 2>/dev/null || echo -e "  DAGs directory not found" && \
	echo -e "\033[0;33mProcess Status:\033[0m\n" && \
	$(MAKE) airflow-status && \
	echo -e "\033[0;33mAirflow Version:\033[0m\n" && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	$(PYTHON) -c "import airflow; print(f'Airflow version: {airflow.__version__}')" 2>/dev/null || \
	echo -e "  Airflow not available"

airflow-webui: ## üåä Open and display Airflow web interface in browser
	@printf "\033[0;36mOpening Airflow web interface...\033[0m\n"
	@if $(MAKE) airflow-status | grep -q "Webserver running"; then \
		echo -e "\033[0;32mAirflow webserver is running\033[0m\n"; \
		echo -e "\033[0;36mOpening http://localhost:8080 in browser...\033[0m\n"; \
		if command -v open >/dev/null 2>&1; then \
			open http://localhost:8080; \
		elif command -v xdg-open >/dev/null 2>&1; then \
			xdg-open http://localhost:8080; \
		elif command -v start >/dev/null 2>&1; then \
			start http://localhost:8080; \
		else \
			echo -e "\033[0;33mPlease manually open: http://localhost:8080\033[0m\n"; \
		fi; \
	else \
		echo -e "\033[0;31mAirflow webserver is not running\033[0m\n"; \
		echo -e "\033[0;33mStart with: make airflow-start\033[0m\n"; \
	fi

# ============================================================================
# AIRFLOW WORKFLOW TESTING
# ============================================================================

.PHONY: airflow-test-connection airflow-validate-dags airflow-test-task

airflow-test-connection: _load_env ## üß™ Execute tests Airflow database connection
	@printf "\033[0;36mTesting Airflow database connection...\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -c "from airflow import settings; settings.engine.execute('SELECT 1'); print('‚úì Airflow database connection successful')" 2>/dev/null || \
	echo -e "\033[0;31m‚ùå Airflow database connection failed\033[0m\n"

airflow-validate-dags: _load_env ## üåä Validate all DAGs for syntax errors
	@printf "\033[0;36mValidating DAG syntax...\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	if [ -d "dags" ]; then \
		for dag_file in dags/*.py; do \
			if [ -f "$$dag_file" ]; then \
				echo -e "\033[0;33mValidating: $$(basename $$dag_file)\033[0m\n"; \
				$(PYTHON) -m py_compile "$$dag_file" && \
				echo -e "\033[0;32m  ‚úì Syntax OK\033[0m\n" || \
				echo -e "\033[0;31m  ‚ùå Syntax Error\033[0m\n"; \
			fi; \
		done; \
	else \
		echo -e "\033[0;33mNo dags directory found\033[0m\n"; \
	fi

airflow-test-task: _load_env ## üß™ Execute tests specific task execution (interactive)
	@printf "\033[0;36mTask Testing\033[0m\n"
	@$(MAKE) airflow-dag-list
	@printf "\033[0;33mEnter DAG ID:\033[0m\n"
	@read -p "DAG ID: " dag_id && \
	echo -e "\033[0;33mEnter Task ID:\033[0m\n" && \
	read -p "Task ID: " task_id && \
	echo -e "\033[0;33mEnter execution date (YYYY-MM-DD) or press Enter for today:\033[0m\n" && \
	read -p "Date: " exec_date && \
	exec_date=$${exec_date:-$$(date +%Y-%m-%d)} && \
	set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow tasks test $$dag_id $$task_id $$exec_date

# ============================================================================
# AIRFLOW CLEANUP AND MAINTENANCE
# ============================================================================

.PHONY: airflow-clean-logs airflow-clean-all

airflow-clean-logs: ## üåä Clean Airflow log files
	@printf "\033[0;33mCleaning Airflow logs...\033[0m\n"
	@rm -rf airflow/logs/*
	@rm -f airflow/airflow-*.out airflow/airflow-*.err
	@printf "\033[0;32m‚úì Airflow logs cleaned\033[0m\n"

airflow-clean-all: airflow-stop ## üåä Clean all Airflow data (DESTRUCTIVE)
	@printf "\033[0;31mWARNING: This will remove all Airflow data!\033[0m\n"
	@printf "\033[0;33mPress Ctrl+C to cancel, or Enter to continue...\033[0m\n"
	@read -p ""
	@printf "\033[0;33mRemoving all Airflow data...\033[0m\n"
	@rm -rf airflow/
	@printf "\033[0;32m‚úì All Airflow data removed\033[0m\n"
	@printf "\033[0;36mRun 'make airflow-init' to reinitialize\033[0m\n"