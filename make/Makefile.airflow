# ============================================================================
# AIRFLOW ORCHESTRATION AND MANAGEMENT
# ============================================================================
# This module handles Apache Airflow operations, including service management,
# DAG operations, and workflow orchestration
#
# Dependencies: Makefile.platform, Makefile.env, Makefile.database
# ============================================================================

# Airflow configuration
AIRFLOW_HOME := $(shell pwd)/airflow
AIRFLOW_DAGS_FOLDER := $(shell pwd)/dags

# ============================================================================
# AIRFLOW SERVICE MANAGEMENT
# ============================================================================

.PHONY: airflow-start airflow-stop airflow-restart airflow-status _check_airflow_deps

airflow-start: _load_env _check_airflow_deps airflow-init ## 🌊 Start Airflow (scheduler + webserver)
	@printf "\033[0;34mStarting Apache Airflow...\033[0m\n"
	@set -a && . ./.env && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__CORE__LOAD_EXAMPLES="false" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	export AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS="$$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS" && \
	export AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE="$$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE" && \
	export AIRFLOW_CONN_POSTGRES_DEFAULT="$$AIRFLOW_CONN_POSTGRES_DEFAULT" && \
	export AIRFLOW__CORE__PARALLELISM=8 && \
	export AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=1 && \
	export AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=8 && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE=5 && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW=10 && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_PRE_PING=True && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_RECYCLE=3600 && \
	export PYTHONPATH="$(AIRFLOW_HOME)/../:$(AIRFLOW_HOME)/../dags:$(AIRFLOW_HOME)/../src:$$PYTHONPATH" && \
	printf "\033[0;33mDebug: LOAD_EXAMPLES = $$AIRFLOW__CORE__LOAD_EXAMPLES\033[0m\n" && \
	printf "\033[0;33mDebug: AUTH_USERS = $$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS\033[0m\n" && \
	printf "\033[0;33mDebug: POSTGRES_CONN = $$AIRFLOW_CONN_POSTGRES_DEFAULT\033[0m\n" && \
	printf "\033[0;32mDebug: CONNECTION POOL CONFIG - SIZE=$$AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE, OVERFLOW=$$AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW\033[0m\n" && \
	printf "\033[0;32mDebug: EXECUTION LIMITS - PARALLELISM=$$AIRFLOW__CORE__PARALLELISM, MAX_RUNS=$$AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG\033[0m\n" && \
	printf "\033[0;33mStarting Airflow scheduler...\033[0m\n" && \
	nohup env \
		AIRFLOW_HOME="$$AIRFLOW_HOME" \
		AIRFLOW__CORE__DAGS_FOLDER="$$AIRFLOW__CORE__DAGS_FOLDER" \
		AIRFLOW__CORE__LOAD_EXAMPLES="$$AIRFLOW__CORE__LOAD_EXAMPLES" \
		AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" \
		AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS="$$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS" \
		AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE="$$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE" \
		AIRFLOW_CONN_POSTGRES_DEFAULT="$$AIRFLOW_CONN_POSTGRES_DEFAULT" \
		AIRFLOW__CORE__PARALLELISM="$$AIRFLOW__CORE__PARALLELISM" \
		AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG="$$AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG" \
		AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG="$$AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG" \
		AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE="$$AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE" \
		AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW="$$AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW" \
		AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_PRE_PING="$$AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_PRE_PING" \
		AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_RECYCLE="$$AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_RECYCLE" \
		PYTHONPATH="$$PYTHONPATH" \
		$(PYTHON) -m airflow scheduler > airflow/airflow-scheduler.out 2> airflow/airflow-scheduler.err & \
	echo $$! > airflow/scheduler.pid && \
	printf "\033[0;33mStarting Airflow API server...\033[0m\n" && \
	nohup env \
		AIRFLOW_HOME="$$AIRFLOW_HOME" \
		AIRFLOW__CORE__DAGS_FOLDER="$$AIRFLOW__CORE__DAGS_FOLDER" \
		AIRFLOW__CORE__LOAD_EXAMPLES="$$AIRFLOW__CORE__LOAD_EXAMPLES" \
		AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" \
		AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS="$$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS" \
		AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE="$$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE" \
		AIRFLOW_CONN_POSTGRES_DEFAULT="$$AIRFLOW_CONN_POSTGRES_DEFAULT" \
		PYTHONPATH="$$PYTHONPATH" \
		$(PYTHON) -m airflow api-server --port 8080 > airflow/airflow-api-server.out 2> airflow/airflow-api-server.err & \
	echo $$! > airflow/api-server.pid && \
	sleep 3 && \
	printf "\033[0;32m✓ Airflow started\033[0m\n" && \
	printf "\033[0;36mWeb UI: http://localhost:8080\033[0m\n" && \
	printf "\033[0;36mPasswords in: airflow/simple_auth_manager_passwords.json\033[0m\n" && \
	printf "\033[0;36mLogs: airflow/airflow-*.out\033[0m\n"

airflow-stop: ## 🌊 Stop all Airflow processes
	@printf "\033[0;34mStopping Apache Airflow...\033[0m\n"
	@if [ -f airflow/scheduler.pid ]; then \
		printf "\033[0;33mStopping Airflow scheduler...\033[0m\n"; \
		kill $$(cat airflow/scheduler.pid) 2>/dev/null || true; \
		rm -f airflow/scheduler.pid; \
	fi
	@if [ -f airflow/webserver.pid ]; then \
		printf "\033[0;33mStopping Airflow webserver...\033[0m\n"; \
		kill $$(cat airflow/webserver.pid) 2>/dev/null || true; \
		rm -f airflow/webserver.pid; \
	fi
	@if [ -f airflow/api-server.pid ]; then \
		printf "\033[0;33mStopping Airflow API server...\033[0m\n"; \
		kill $$(cat airflow/api-server.pid) 2>/dev/null || true; \
		rm -f airflow/api-server.pid; \
	fi
	@if [ -f airflow/standalone.pid ]; then \
		printf "\033[0;33mStopping Airflow standalone...\033[0m\n"; \
		kill $$(cat airflow/standalone.pid) 2>/dev/null || true; \
		rm -f airflow/standalone.pid; \
	fi
	@pkill -f "airflow scheduler" 2>/dev/null || true
	@pkill -f "airflow webserver" 2>/dev/null || true
	@pkill -f "airflow api-server" 2>/dev/null || true
	@pkill -f "airflow api_server" 2>/dev/null || true
	@pkill -f "airflow worker" 2>/dev/null || true
	@pkill -f "airflow standalone" 2>/dev/null || true
	@pkill -f "python.*airflow" 2>/dev/null || true
	@printf "\033[0;33mWaiting 3 seconds for graceful shutdown...\033[0m\n"
	@sleep 3
	@if pgrep -f "airflow" >/dev/null 2>&1; then \
		printf "\033[0;33mForce killing remaining Airflow processes...\033[0m\n"; \
		pkill -9 -f "airflow scheduler" 2>/dev/null || true; \
		pkill -9 -f "airflow webserver" 2>/dev/null || true; \
		pkill -9 -f "airflow api-server" 2>/dev/null || true; \
		pkill -9 -f "airflow api_server" 2>/dev/null || true; \
		pkill -9 -f "airflow worker" 2>/dev/null || true; \
		pkill -9 -f "airflow standalone" 2>/dev/null || true; \
		pkill -9 -f "python.*airflow" 2>/dev/null || true; \
		sleep 2; \
	fi
	@if pgrep -f "airflow" >/dev/null 2>&1; then \
		printf "\033[0;31m⚠ Warning: Some Airflow processes may still be running\033[0m\n"; \
		printf "\033[0;33mRemaining processes:\033[0m\n"; \
		pgrep -f "airflow" | head -5 || true; \
	fi
	@printf "\033[0;32m✓ Airflow stopped\033[0m\n"

airflow-restart: airflow-stop airflow-start ## 🌊 Restart Airflow services
	@printf "\033[0;32m✓ Airflow restarted\033[0m\n"

airflow-status: ## 🌊 Validate Airflow process status
	@printf "\033[0;36mAirflow process status:\033[0m\n"
	@if [ -f airflow/scheduler.pid ] && kill -0 $$(cat airflow/scheduler.pid) 2>/dev/null; then \
		printf "\033[0;32m✓ Scheduler running (PID: $$(cat airflow/scheduler.pid))\033[0m\n"; \
	else \
		printf "\033[0;33m⚠ Scheduler not running\033[0m\n"; \
	fi
	@if [ -f airflow/webserver.pid ] && kill -0 $$(cat airflow/webserver.pid) 2>/dev/null; then \
		printf "\033[0;32m✓ Webserver running (PID: $$(cat airflow/webserver.pid))\033[0m\n"; \
	elif [ -f airflow/api-server.pid ] && kill -0 $$(cat airflow/api-server.pid) 2>/dev/null; then \
		printf "\033[0;32m✓ API Server running (PID: $$(cat airflow/api-server.pid))\033[0m\n"; \
	else \
		printf "\033[0;33m⚠ Webserver/API Server not running\033[0m\n"; \
	fi
	@if pgrep -f "airflow" >/dev/null; then \
		printf "\033[0;32mActive Airflow processes:\033[0m\n"; \
		pgrep -f "airflow" -l; \
	else \
		printf "\033[0;33m⚠ No Airflow processes found\033[0m\n"; \
	fi

# ============================================================================
# AIRFLOW INITIALIZATION AND CONFIGURATION
# ============================================================================

.PHONY: airflow-init airflow-reset airflow-config _setup_airflow_dirs _init_airflow_db

airflow-init: _load_env _setup_airflow_dirs _init_airflow_db ## 🌊 Initialize Airflow database (Simple auth manager auto-creates users)
	@printf "\033[0;34mInitializing Apache Airflow with Simple Auth Manager...\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__CORE__LOAD_EXAMPLES="false" && \
	export AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS="$$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS" && \
	export AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE="$$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	printf "\\033[0;33mSimple Auth Manager will auto-create users: $$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS\\033[0m\\n" && \
	printf "\\033[0;32m✓ Airflow initialization completed\\033[0m\\n"

airflow-reset: _load_env ## 🌊 Reset Airflow configuration and database (DESTRUCTIVE)
	@printf "\033[0;31mWARNING: This will reset all Airflow data!\033[0m\n"
	@printf "\033[0;33mPress Ctrl+C to cancel, or Enter to continue...\033[0m\n"
	@read -p ""
	@$(MAKE) airflow-stop
	@printf "\033[0;33mRemoving Airflow database and logs...\033[0m\n"
	@rm -rf airflow/airflow.db airflow/logs/* airflow/airflow-*.out airflow/airflow-*.err
	@$(MAKE) airflow-init
	@printf "\033[0;32m✓ Airflow reset completed\033[0m\n"

airflow-config: _load_env ## 🌊 Display Airflow configuration
	@printf "\033[0;36mAirflow Configuration:\033[0m\n"
	@set -a && . ./.env && set +a && \
	printf "\\033[0;33mEnvironment Variables:\\033[0m\\n" && \
	printf "  AIRFLOW_HOME: $(AIRFLOW_HOME)\\n" && \
	printf "  AIRFLOW__CORE__DAGS_FOLDER: $(AIRFLOW_DAGS_FOLDER)\\n" && \
	printf "  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: $$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\\n" && \
	printf "  AIRFLOW_ADMIN_USERNAME: $$AIRFLOW_ADMIN_USERNAME\\n" && \
	printf "\\033[0;33mAdmin Configuration:\\033[0m\\n" && \
	printf "  Username: $$AIRFLOW_ADMIN_USERNAME\\n" && \
	printf "  Email: $$AIRFLOW_ADMIN_EMAIL\\n" && \
	printf "  First Name: $$AIRFLOW_ADMIN_FIRSTNAME\\n" && \
	printf "  Last Name: $$AIRFLOW_ADMIN_LASTNAME\\n"

_setup_airflow_dirs: ## Internal: Create Airflow directory structure
	@printf "\033[0;33mSetting up Airflow directories...\033[0m\n"
	@mkdir -p airflow/logs airflow/plugins
	@printf "\033[0;32m✓ Airflow directories created\033[0m\n"

_init_airflow_db: _load_env ## Internal: Initialize Airflow database
	@printf "\033[0;33mInitializing Airflow database...\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__CORE__LOAD_EXAMPLES="false" && \
	export AIRFLOW__CORE__AUTH_MANAGER="airflow.api_fastapi.auth.managers.simple.simple_auth_manager.SimpleAuthManager" && \
	export AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS="$$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS" && \
	export AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE="$$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	export AIRFLOW__API_AUTH__JWT_SECRET="$$AIRFLOW__API_AUTH__JWT_SECRET" && \
	export PYTHONPATH="$$PYTHONPATH" && \
	env AIRFLOW_HOME="$(AIRFLOW_HOME)" \
		AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" \
		AIRFLOW__CORE__LOAD_EXAMPLES="false" \
		AIRFLOW__CORE__AUTH_MANAGER="airflow.api_fastapi.auth.managers.simple.simple_auth_manager.SimpleAuthManager" \
		AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS="$$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS" \
		AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE="$$AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE" \
		AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" \
		AIRFLOW__API_AUTH__JWT_SECRET="$$AIRFLOW__API_AUTH__JWT_SECRET" \
		PYTHONPATH="$$PYTHONPATH" \
		$(PYTHON) -m airflow db migrate

# ============================================================================
# DAG MANAGEMENT AND OPERATIONS
# ============================================================================

.PHONY: airflow-dag-list airflow-dag-test airflow-dag-trigger airflow-dag-info

airflow-dag-list: _load_env ## 🌊 List and display available DAGs
	@printf "\033[0;36mAvailable DAGs:\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow dags list

airflow-dag-test: _load_env ## 🧪 Execute tests DAG execution (interactive)
	@printf "\033[0;36mDAG Testing\033[0m\n"
	@$(MAKE) airflow-dag-list
	@printf "\033[0;33mEnter DAG ID to test:\033[0m\n"
	@read -p "DAG ID: " dag_id && \
	printf "\033[0;33mEnter execution date (YYYY-MM-DD) or press Enter for today:\033[0m\n" && \
	read -p "Date: " exec_date && \
	exec_date=$${exec_date:-$$(date +%Y-%m-%d)} && \
	set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow dags test $$dag_id $$exec_date

airflow-dag-trigger: _load_env ## 🌊 Manually trigger DAG run (interactive)
	@printf "\033[0;36mDAG Triggering\033[0m\n"
	@$(MAKE) airflow-dag-list
	@printf "\033[0;33mEnter DAG ID to trigger:\033[0m\n"
	@read -p "DAG ID: " dag_id && \
	set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow dags trigger $$dag_id && \
	printf "\033[0;32m✓ DAG $$dag_id triggered\033[0m\n"

airflow-dag-info: _load_env ## 🌊 Display DAG information (interactive)
	@printf "\033[0;36mDAG Information\033[0m\n"
	@$(MAKE) airflow-dag-list
	@printf "\033[0;33mEnter DAG ID for information:\033[0m\n"
	@read -p "DAG ID: " dag_id && \
	set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow dags show $$dag_id

# ============================================================================
# AIRFLOW UTILITIES AND DEBUGGING
# ============================================================================

.PHONY: airflow-shell airflow-logs airflow-debug airflow-webui

airflow-shell: _load_env ## 🌊 Open and display Airflow shell for debugging
	@printf "\033[0;36mOpening Airflow shell...\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -c "import airflow; from airflow import settings; print('Airflow shell ready. Access via settings object.'); import IPython; IPython.embed()" 2>/dev/null || \
	$(PYTHON) -c "import airflow; from airflow import settings; print('Airflow shell ready. Use settings object for configuration.')"

airflow-logs: ## 🌊 View Airflow logs
	@printf "\033[0;36mAirflow logs:\033[0m\n"
	@printf "\033[0;33mRecent scheduler logs:\033[0m\n"
	@tail -n 20 airflow/airflow-scheduler.out 2>/dev/null || printf "\033[0;33mNo scheduler logs found\033[0m\n"
	@printf "\033[0;33mRecent webserver logs:\033[0m\n"
	@tail -n 20 airflow/airflow-webserver.out 2>/dev/null || printf "\033[0;33mNo webserver logs found\033[0m\n"
	@printf "\033[0;33mRecent error logs:\033[0m\n"
	@tail -n 10 airflow/airflow-scheduler.err 2>/dev/null || printf "\033[0;33mNo scheduler errors\033[0m\n"
	@tail -n 10 airflow/airflow-webserver.err 2>/dev/null || printf "\033[0;33mNo webserver errors\033[0m\n"

airflow-debug: _load_env ## 🌊 Debug and troubleshoot Airflow configuration and setup
	@printf "\033[0;36mAirflow Debug Information:\033[0m\n"
	@set -a && . ./.env && set +a && \
	printf "\033[0;33mEnvironment Configuration:\033[0m\n" && \
	echo -e "  AIRFLOW_HOME: $(AIRFLOW_HOME)" && \
	echo -e "  AIRFLOW__CORE__DAGS_FOLDER: $(AIRFLOW_DAGS_FOLDER)" && \
	echo -e "  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: $$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	printf "\033[0;33mDirectory Structure:\033[0m\n" && \
	ls -la airflow/ 2>/dev/null || echo -e "  Airflow directory not found" && \
	printf "\033[0;33mDAGs Directory:\033[0m\n" && \
	ls -la dags/ 2>/dev/null || echo -e "  DAGs directory not found" && \
	printf "\033[0;33mProcess Status:\033[0m\n" && \
	$(MAKE) airflow-status && \
	printf "\033[0;33mAirflow Version:\033[0m\n" && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	$(PYTHON) -c "import airflow; print(f'Airflow version: {airflow.__version__}')" 2>/dev/null || \
	echo -e "  Airflow not available"

airflow-webui: ## 🌊 Open and display Airflow web interface in browser
	@printf "\033[0;36mOpening Airflow web interface...\033[0m\n"
	@if $(MAKE) airflow-status | grep -q "Webserver running"; then \
		printf "\033[0;32mAirflow webserver is running\033[0m\n"; \
		printf "\033[0;36mOpening http://localhost:8080 in browser...\033[0m\n"; \
		if command -v open >/dev/null 2>&1; then \
			open http://localhost:8080; \
		elif command -v xdg-open >/dev/null 2>&1; then \
			xdg-open http://localhost:8080; \
		elif command -v start >/dev/null 2>&1; then \
			start http://localhost:8080; \
		else \
			printf "\033[0;33mPlease manually open: http://localhost:8080\033[0m\n"; \
		fi; \
	else \
		printf "\033[0;31mAirflow webserver is not running\033[0m\n"; \
		printf "\033[0;33mStart with: make airflow-start\033[0m\n"; \
	fi

# ============================================================================
# AIRFLOW WORKFLOW TESTING
# ============================================================================

.PHONY: airflow-test-connection airflow-validate-dags airflow-test-task

airflow-test-connection: _load_env ## 🧪 Execute tests Airflow database connection
	@printf "\033[0;36mTesting Airflow database connection...\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -c "from airflow import settings; settings.engine.execute('SELECT 1'); print('✓ Airflow database connection successful')" 2>/dev/null || \
	printf "\033[0;31m❌ Airflow database connection failed\033[0m\n"

airflow-validate-dags: _load_env ## 🌊 Validate all DAGs for syntax errors
	@printf "\033[0;36mValidating DAG syntax...\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	if [ -d "dags" ]; then \
		for dag_file in dags/*.py; do \
			if [ -f "$$dag_file" ]; then \
				printf "\033[0;33mValidating: $$(basename $$dag_file)\033[0m\n"; \
				$(PYTHON) -m py_compile "$$dag_file" && \
				printf "\033[0;32m  ✓ Syntax OK\033[0m\n" || \
				printf "\033[0;31m  ❌ Syntax Error\033[0m\n"; \
			fi; \
		done; \
	else \
		printf "\033[0;33mNo dags directory found\033[0m\n"; \
	fi

airflow-monitor-connections: _load_env ## 📊 Monitor database connections and worker processes (PRODUCTION CRITICAL)
	@printf "\033[0;34m=== CONNECTION POOL MONITORING ===\033[0m\n"
	@printf "\033[0;33mWorker Processes:\033[0m\n"
	@ps aux | grep "airflow worker" | grep -v grep | wc -l | awk '{printf "Active Workers: %s\n", $$1}'
	@ps aux | grep "airflow worker" | grep -v grep | head -5
	@printf "\033[0;33m\nPostgreSQL Connections:\033[0m\n"
	@set -a && . ./.env && set +a && \
	psql "$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" -c \
		"SELECT datname, usename, application_name, state, backend_start FROM pg_stat_activity WHERE datname LIKE '%ticker%' ORDER BY backend_start;" 2>/dev/null || \
		printf "\033[0;31mError: Cannot connect to database\033[0m\n"
	@printf "\033[0;33m\nDAG Status:\033[0m\n"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	export PYTHONPATH="$(AIRFLOW_HOME)/../:$(AIRFLOW_HOME)/../dags:$(AIRFLOW_HOME)/../src:$$PYTHONPATH" && \
	$(PYTHON) -m airflow dags list-runs daily_etl_pipeline --limit 3 2>/dev/null || printf "\033[0;31mAirflow CLI unavailable\033[0m\n"

airflow-emergency-clear: _load_env ## 🚨 Clear all running/failed tasks (EMERGENCY USE ONLY)
	@printf "\033[0;31m⚠️  EMERGENCY: Clearing all hanging tasks\033[0m\n"
	@read -p "Are you sure? This will clear ALL running tasks [y/N]: " confirm && \
	if [ "$$confirm" = "y" ] || [ "$$confirm" = "Y" ]; then \
		set -a && . ./.env && set +a && \
		export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
		export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
		export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
		export PYTHONPATH="$(AIRFLOW_HOME)/../:$(AIRFLOW_HOME)/../dags:$(AIRFLOW_HOME)/../src:$$PYTHONPATH" && \
		printf "\033[0;33mClearing daily_etl_pipeline...\033[0m\n" && \
		$(PYTHON) -m airflow tasks clear daily_etl_pipeline -r -y && \
		printf "\033[0;33mClearing test_etl_dag...\033[0m\n" && \
		$(PYTHON) -m airflow tasks clear test_etl_dag -r -y && \
		printf "\033[0;32m✓ Emergency clear completed\033[0m\n"; \
	else \
		printf "\033[0;33mOperation cancelled\033[0m\n"; \
	fi

airflow-test-task: _load_env ## 🧪 Execute tests specific task execution (interactive)
	@printf "\033[0;36mTask Testing\033[0m\n"
	@$(MAKE) airflow-dag-list
	@printf "\033[0;33mEnter DAG ID:\033[0m\n"
	@read -p "DAG ID: " dag_id && \
	printf "\033[0;33mEnter Task ID:\033[0m\n" && \
	read -p "Task ID: " task_id && \
	printf "\033[0;33mEnter execution date (YYYY-MM-DD) or press Enter for today:\033[0m\n" && \
	read -p "Date: " exec_date && \
	exec_date=$${exec_date:-$$(date +%Y-%m-%d)} && \
	set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow tasks test $$dag_id $$task_id $$exec_date

# ============================================================================
# AIRFLOW CLEANUP AND MAINTENANCE
# ============================================================================

.PHONY: airflow-clean-logs airflow-clean-all

airflow-clean-logs: ## 🌊 Clean Airflow log files
	@printf "\033[0;33mCleaning Airflow logs...\033[0m\n"
	@rm -rf airflow/logs/*
	@rm -f airflow/airflow-*.out airflow/airflow-*.err
	@printf "\033[0;32m✓ Airflow logs cleaned\033[0m\n"

airflow-clean-all: airflow-stop ## 🌊 Clean all Airflow data (DESTRUCTIVE)
	@printf "\033[0;31mWARNING: This will remove all Airflow data!\033[0m\n"
	@printf "\033[0;33mPress Ctrl+C to cancel, or Enter to continue...\033[0m\n"
	@read -p ""
	@printf "\033[0;33mRemoving all Airflow data...\033[0m\n"
	@rm -rf airflow/
	@printf "\033[0;32m✓ All Airflow data removed\033[0m\n"
	@printf "\033[0;36mRun 'make airflow-init' to reinitialize\033[0m\n"
