# ============================================================================
# AIRFLOW ORCHESTRATION AND MANAGEMENT
# ============================================================================
# This module handles Apache Airflow operations, including service management,
# DAG operations, and workflow orchestration
#
# Dependencies: Makefile.platform, Makefile.env, Makefile.database
# ============================================================================

# Airflow configuration
AIRFLOW_HOME := $(shell pwd)/airflow
AIRFLOW_DAGS_FOLDER := $(shell pwd)/dags

# ============================================================================
# AIRFLOW SERVICE MANAGEMENT
# ============================================================================

.PHONY: airflow-start airflow-stop airflow-restart airflow-status _check_airflow_deps

airflow-start: _load_env _check_airflow_deps airflow-init ## Start Airflow (scheduler + webserver)
	@echo -e "$(BLUE)Starting Apache Airflow...$(NC)"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__CORE__LOAD_EXAMPLES="false" && \
	export AIRFLOW__CORE__AUTH_MANAGER="airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	export AIRFLOW__API_AUTH__JWT_SECRET="$$AIRFLOW__API_AUTH__JWT_SECRET" && \
	echo -e "$(YELLOW)Starting Airflow scheduler...$(NC)" && \
	nohup $(PYTHON) -m airflow scheduler > airflow/airflow-scheduler.out 2> airflow/airflow-scheduler.err & \
	echo $$! > airflow/scheduler.pid && \
	echo -e "$(YELLOW)Starting Airflow API server...$(NC)" && \
	nohup $(PYTHON) -m airflow api-server --port 8080 > airflow/airflow-webserver.out 2> airflow/airflow-webserver.err & \
	echo $$! > airflow/webserver.pid && \
	sleep 3 && \
	echo -e "$(GREEN)✓ Airflow started$(NC)" && \
	echo -e "$(CYAN)API Server: http://localhost:8080$(NC)" && \
	echo -e "$(CYAN)Logs: airflow/airflow-*.out$(NC)"

airflow-stop: ## Stop all Airflow processes
	@echo -e "$(BLUE)Stopping Apache Airflow...$(NC)"
	@if [ -f airflow/scheduler.pid ]; then \
		echo -e "$(YELLOW)Stopping Airflow scheduler...$(NC)"; \
		kill $$(cat airflow/scheduler.pid) 2>/dev/null || true; \
		rm -f airflow/scheduler.pid; \
	fi
	@if [ -f airflow/webserver.pid ]; then \
		echo -e "$(YELLOW)Stopping Airflow webserver...$(NC)"; \
		kill $$(cat airflow/webserver.pid) 2>/dev/null || true; \
		rm -f airflow/webserver.pid; \
	fi
	@pkill -f "airflow scheduler" 2>/dev/null || true
	@pkill -f "airflow webserver" 2>/dev/null || true
	@echo -e "$(GREEN)✓ Airflow stopped$(NC)"

airflow-restart: airflow-stop airflow-start ## Restart Airflow services
	@echo -e "$(GREEN)✓ Airflow restarted$(NC)"

airflow-status: ## Check Airflow process status
	@echo -e "$(CYAN)Airflow process status:$(NC)"
	@if [ -f airflow/scheduler.pid ] && kill -0 $$(cat airflow/scheduler.pid) 2>/dev/null; then \
		echo -e "$(GREEN)✓ Scheduler running (PID: $$(cat airflow/scheduler.pid))$(NC)"; \
	else \
		echo -e "$(YELLOW)⚠ Scheduler not running$(NC)"; \
	fi
	@if [ -f airflow/webserver.pid ] && kill -0 $$(cat airflow/webserver.pid) 2>/dev/null; then \
		echo -e "$(GREEN)✓ Webserver running (PID: $$(cat airflow/webserver.pid))$(NC)"; \
	else \
		echo -e "$(YELLOW)⚠ Webserver not running$(NC)"; \
	fi
	@if pgrep -f "airflow" >/dev/null; then \
		echo -e "$(GREEN)Active Airflow processes:$(NC)"; \
		pgrep -f "airflow" -l; \
	else \
		echo -e "$(YELLOW)⚠ No Airflow processes found$(NC)"; \
	fi

_check_airflow_deps: ## Internal: Check Airflow dependencies
	@echo -e "$(YELLOW)Checking Airflow dependencies...$(NC)"
	@$(PYTHON) -c "import airflow" 2>/dev/null || \
	(echo -e "$(RED)❌ Airflow not installed. Run 'make install-dev' first.$(NC)" && exit 1)
	@echo -e "$(GREEN)✓ Airflow dependencies available$(NC)"

# ============================================================================
# AIRFLOW INITIALIZATION AND CONFIGURATION
# ============================================================================

.PHONY: airflow-init airflow-reset airflow-config _setup_airflow_dirs _init_airflow_db

airflow-init: _load_env _setup_airflow_dirs _init_airflow_db ## Initialize Airflow database and admin user
	@echo -e "$(BLUE)Initializing Apache Airflow...$(NC)"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__CORE__LOAD_EXAMPLES="false" && \
	export AIRFLOW__CORE__AUTH_MANAGER="airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	export AIRFLOW__API_AUTH__JWT_SECRET="$$AIRFLOW__API_AUTH__JWT_SECRET" && \
	echo -e "$(YELLOW)Creating Airflow admin user...$(NC)" && \
	$(PYTHON) -m airflow users create \
		--username "$$AIRFLOW_ADMIN_USERNAME" \
		--firstname "$$AIRFLOW_ADMIN_FIRSTNAME" \
		--lastname "$$AIRFLOW_ADMIN_LASTNAME" \
		--role Admin \
		--email "$$AIRFLOW_ADMIN_EMAIL" \
		--password "$$AIRFLOW_ADMIN_PASSWORD" \
		2>/dev/null || echo -e "$(YELLOW)⚠ Admin user already exists or creation failed$(NC)" && \
	echo -e "$(GREEN)✓ Airflow initialization completed$(NC)"

airflow-reset: _load_env ## Reset Airflow configuration and database (DESTRUCTIVE)
	@echo -e "$(RED)WARNING: This will reset all Airflow data!$(NC)"
	@echo -e "$(YELLOW)Press Ctrl+C to cancel, or Enter to continue...$(NC)"
	@read -p ""
	@$(MAKE) airflow-stop
	@echo -e "$(YELLOW)Removing Airflow database and logs...$(NC)"
	@rm -rf airflow/airflow.db airflow/logs/* airflow/airflow-*.out airflow/airflow-*.err
	@$(MAKE) airflow-init
	@echo -e "$(GREEN)✓ Airflow reset completed$(NC)"

airflow-config: _load_env ## Display Airflow configuration
	@echo -e "$(CYAN)Airflow Configuration:$(NC)"
	@set -a && . ./.env && set +a && \
	echo -e "$(YELLOW)Environment Variables:$(NC)" && \
	echo -e "  AIRFLOW_HOME: $(AIRFLOW_HOME)" && \
	echo -e "  AIRFLOW__CORE__DAGS_FOLDER: $(AIRFLOW_DAGS_FOLDER)" && \
	echo -e "  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: $$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	echo -e "  AIRFLOW_ADMIN_USERNAME: $$AIRFLOW_ADMIN_USERNAME" && \
	echo -e "$(YELLOW)Admin Configuration:$(NC)" && \
	echo -e "  Username: $$AIRFLOW_ADMIN_USERNAME" && \
	echo -e "  Email: $$AIRFLOW_ADMIN_EMAIL" && \
	echo -e "  First Name: $$AIRFLOW_ADMIN_FIRSTNAME" && \
	echo -e "  Last Name: $$AIRFLOW_ADMIN_LASTNAME"

_setup_airflow_dirs: ## Internal: Create Airflow directory structure
	@echo -e "$(YELLOW)Setting up Airflow directories...$(NC)"
	@mkdir -p airflow/logs airflow/plugins
	@echo -e "$(GREEN)✓ Airflow directories created$(NC)"

_init_airflow_db: _load_env ## Internal: Initialize Airflow database
	@echo -e "$(YELLOW)Initializing Airflow database...$(NC)"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__CORE__LOAD_EXAMPLES="false" && \
	export AIRFLOW__CORE__AUTH_MANAGER="airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	export AIRFLOW__API_AUTH__JWT_SECRET="$$AIRFLOW__API_AUTH__JWT_SECRET" && \
	$(PYTHON) -m airflow db migrate

# ============================================================================
# DAG MANAGEMENT AND OPERATIONS
# ============================================================================

.PHONY: airflow-dag-list airflow-dag-test airflow-dag-trigger airflow-dag-info

airflow-dag-list: _load_env ## List available DAGs
	@echo -e "$(CYAN)Available DAGs:$(NC)"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow dags list

airflow-dag-test: _load_env ## Test DAG execution (interactive)
	@echo -e "$(CYAN)DAG Testing$(NC)"
	@$(MAKE) airflow-dag-list
	@echo -e "$(YELLOW)Enter DAG ID to test:$(NC)"
	@read -p "DAG ID: " dag_id && \
	echo -e "$(YELLOW)Enter execution date (YYYY-MM-DD) or press Enter for today:$(NC)" && \
	read -p "Date: " exec_date && \
	exec_date=$${exec_date:-$$(date +%Y-%m-%d)} && \
	set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow dags test $$dag_id $$exec_date

airflow-dag-trigger: _load_env ## Manually trigger DAG run (interactive)
	@echo -e "$(CYAN)DAG Triggering$(NC)"
	@$(MAKE) airflow-dag-list
	@echo -e "$(YELLOW)Enter DAG ID to trigger:$(NC)"
	@read -p "DAG ID: " dag_id && \
	set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow dags trigger $$dag_id && \
	echo -e "$(GREEN)✓ DAG $$dag_id triggered$(NC)"

airflow-dag-info: _load_env ## Show DAG information (interactive)
	@echo -e "$(CYAN)DAG Information$(NC)"
	@$(MAKE) airflow-dag-list
	@echo -e "$(YELLOW)Enter DAG ID for information:$(NC)"
	@read -p "DAG ID: " dag_id && \
	set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow dags show $$dag_id

# ============================================================================
# AIRFLOW UTILITIES AND DEBUGGING
# ============================================================================

.PHONY: airflow-shell airflow-logs airflow-debug airflow-webui

airflow-shell: _load_env ## Open Airflow shell for debugging
	@echo -e "$(CYAN)Opening Airflow shell...$(NC)"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -c "import airflow; from airflow import settings; print('Airflow shell ready. Access via settings object.'); import IPython; IPython.embed()" 2>/dev/null || \
	$(PYTHON) -c "import airflow; from airflow import settings; print('Airflow shell ready. Use settings object for configuration.')"

airflow-logs: ## View Airflow logs
	@echo -e "$(CYAN)Airflow logs:$(NC)"
	@echo -e "$(YELLOW)Recent scheduler logs:$(NC)"
	@tail -n 20 airflow/airflow-scheduler.out 2>/dev/null || echo -e "$(YELLOW)No scheduler logs found$(NC)"
	@echo -e "$(YELLOW)Recent webserver logs:$(NC)"
	@tail -n 20 airflow/airflow-webserver.out 2>/dev/null || echo -e "$(YELLOW)No webserver logs found$(NC)"
	@echo -e "$(YELLOW)Recent error logs:$(NC)"
	@tail -n 10 airflow/airflow-scheduler.err 2>/dev/null || echo -e "$(YELLOW)No scheduler errors$(NC)"
	@tail -n 10 airflow/airflow-webserver.err 2>/dev/null || echo -e "$(YELLOW)No webserver errors$(NC)"

airflow-debug: _load_env ## Debug Airflow configuration and setup
	@echo -e "$(CYAN)Airflow Debug Information:$(NC)"
	@set -a && . ./.env && set +a && \
	echo -e "$(YELLOW)Environment Configuration:$(NC)" && \
	echo -e "  AIRFLOW_HOME: $(AIRFLOW_HOME)" && \
	echo -e "  AIRFLOW__CORE__DAGS_FOLDER: $(AIRFLOW_DAGS_FOLDER)" && \
	echo -e "  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: $$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	echo -e "$(YELLOW)Directory Structure:$(NC)" && \
	ls -la airflow/ 2>/dev/null || echo -e "  Airflow directory not found" && \
	echo -e "$(YELLOW)DAGs Directory:$(NC)" && \
	ls -la dags/ 2>/dev/null || echo -e "  DAGs directory not found" && \
	echo -e "$(YELLOW)Process Status:$(NC)" && \
	$(MAKE) airflow-status && \
	echo -e "$(YELLOW)Airflow Version:$(NC)" && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	$(PYTHON) -c "import airflow; print(f'Airflow version: {airflow.__version__}')" 2>/dev/null || \
	echo -e "  Airflow not available"

airflow-webui: ## Open Airflow web interface in browser
	@echo -e "$(CYAN)Opening Airflow web interface...$(NC)"
	@if $(MAKE) airflow-status | grep -q "Webserver running"; then \
		echo -e "$(GREEN)Airflow webserver is running$(NC)"; \
		echo -e "$(CYAN)Opening http://localhost:8080 in browser...$(NC)"; \
		if command -v open >/dev/null 2>&1; then \
			open http://localhost:8080; \
		elif command -v xdg-open >/dev/null 2>&1; then \
			xdg-open http://localhost:8080; \
		elif command -v start >/dev/null 2>&1; then \
			start http://localhost:8080; \
		else \
			echo -e "$(YELLOW)Please manually open: http://localhost:8080$(NC)"; \
		fi; \
	else \
		echo -e "$(RED)Airflow webserver is not running$(NC)"; \
		echo -e "$(YELLOW)Start with: make airflow-start$(NC)"; \
	fi

# ============================================================================
# AIRFLOW WORKFLOW TESTING
# ============================================================================

.PHONY: airflow-test-connection airflow-validate-dags airflow-test-task

airflow-test-connection: _load_env ## Test Airflow database connection
	@echo -e "$(CYAN)Testing Airflow database connection...$(NC)"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -c "from airflow import settings; settings.engine.execute('SELECT 1'); print('✓ Airflow database connection successful')" 2>/dev/null || \
	echo -e "$(RED)❌ Airflow database connection failed$(NC)"

airflow-validate-dags: _load_env ## Validate all DAGs for syntax errors
	@echo -e "$(CYAN)Validating DAG syntax...$(NC)"
	@set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	if [ -d "dags" ]; then \
		for dag_file in dags/*.py; do \
			if [ -f "$$dag_file" ]; then \
				echo -e "$(YELLOW)Validating: $$(basename $$dag_file)$(NC)"; \
				$(PYTHON) -m py_compile "$$dag_file" && \
				echo -e "$(GREEN)  ✓ Syntax OK$(NC)" || \
				echo -e "$(RED)  ❌ Syntax Error$(NC)"; \
			fi; \
		done; \
	else \
		echo -e "$(YELLOW)No dags directory found$(NC)"; \
	fi

airflow-test-task: _load_env ## Test specific task execution (interactive)
	@echo -e "$(CYAN)Task Testing$(NC)"
	@$(MAKE) airflow-dag-list
	@echo -e "$(YELLOW)Enter DAG ID:$(NC)"
	@read -p "DAG ID: " dag_id && \
	echo -e "$(YELLOW)Enter Task ID:$(NC)" && \
	read -p "Task ID: " task_id && \
	echo -e "$(YELLOW)Enter execution date (YYYY-MM-DD) or press Enter for today:$(NC)" && \
	read -p "Date: " exec_date && \
	exec_date=$${exec_date:-$$(date +%Y-%m-%d)} && \
	set -a && . ./.env && set +a && \
	export AIRFLOW_HOME="$(AIRFLOW_HOME)" && \
	export AIRFLOW__CORE__DAGS_FOLDER="$(AIRFLOW_DAGS_FOLDER)" && \
	export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="$$AIRFLOW__DATABASE__SQL_ALCHEMY_CONN" && \
	$(PYTHON) -m airflow tasks test $$dag_id $$task_id $$exec_date

# ============================================================================
# AIRFLOW CLEANUP AND MAINTENANCE
# ============================================================================

.PHONY: airflow-clean-logs airflow-clean-all

airflow-clean-logs: ## Clean Airflow log files
	@echo -e "$(YELLOW)Cleaning Airflow logs...$(NC)"
	@rm -rf airflow/logs/*
	@rm -f airflow/airflow-*.out airflow/airflow-*.err
	@echo -e "$(GREEN)✓ Airflow logs cleaned$(NC)"

airflow-clean-all: airflow-stop ## Clean all Airflow data (DESTRUCTIVE)
	@echo -e "$(RED)WARNING: This will remove all Airflow data!$(NC)"
	@echo -e "$(YELLOW)Press Ctrl+C to cancel, or Enter to continue...$(NC)"
	@read -p ""
	@echo -e "$(YELLOW)Removing all Airflow data...$(NC)"
	@rm -rf airflow/
	@echo -e "$(GREEN)✓ All Airflow data removed$(NC)"
	@echo -e "$(CYAN)Run 'make airflow-init' to reinitialize$(NC)"
