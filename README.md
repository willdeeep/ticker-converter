# Financial Market Data Analytics Pipeline

A modern Python-based stock ticker conversion and analysis pipeline with Makefile-driven workflow, featuring FastAPI endpoints, PostgreSQL integration, and comprehensive code quality tools.

## Overview

This project implements a production-ready financial data analytics pipeline designed for professional development workflows. It combines SQL-first architecture with modern Python tooling to deliver reliable stock performance analysis.

### Key Features

- **Makefile-Based Workflow**: Simplified operations with intuitive `make` commands
- **FastAPI Integration**: REST API endpoints for stock performance analysis  
- **PostgreSQL Backend**: Robust data persistence with optimized SQL queries
- **Code Quality Tools**: Automated formatting, linting, and type checking
- **Comprehensive Testing**: 69% test coverage with pytest, coverage reporting, and 100% test success rate
- **Development Environment**: Pre-commit hooks and automated quality gates

## What's New in v3.2.1

### USD/GBP Stock Data API Integration
- **New Currency Conversion Endpoint**: Added `/api/stocks/data-with-currency` endpoint providing side-by-side USD and GBP stock prices
- **Real-Time Exchange Rate Integration**: Automatic currency conversion using daily USD/GBP exchange rates
- **Comprehensive Integration Testing**: True database integration tests with PostgreSQL connectivity and data validation
- **Test-Driven Development**: Complete TDD implementation with unit tests for models and SQL, plus integration tests for end-to-end verification
- **Enhanced API Models**: New `StockDataWithCurrency` Pydantic model with field validation for price and volume constraints
- **Production-Ready SQL Queries**: Optimized SQL with proper joins, COALESCE for null handling, and parameterized inputs

### Development Quality Enhancements
- **Code Quality Compliance**: All changes formatted with Black, validated with Pylint 10.00/10, and type-checked with MyPy
- **Separated Test Architecture**: Clean separation of unit tests (models/SQL) and integration tests (real database connections)
- **Data Freshness Validation**: Integration tests verify most recent data retrieval and proper date filtering
- **Error Handling Coverage**: Comprehensive testing for missing exchange rate data and graceful degradation

## What's New in v3.2.0

### Major Performance Optimization and Infrastructure Improvements
- **85% Faster Quality Gate Pipeline**: Reduced from 7m47s to 1m9s through comprehensive test optimization
- **88% Faster Test Suite**: Optimized from 7m29s to 52s by mocking time.sleep() in API client error handling tests  
- **99.98% Individual Test Improvements**: Slowest tests (3+ minutes) now complete in milliseconds
- **Enhanced Airflow DAG Reliability**: Fixed hanging issues with timeout protection and graceful PostgreSQL connection fallbacks
- **SQL Quality Improvements**: Resolved sqlfluff configuration issues and SQL keyword identifier conflicts

### Development Experience Enhancements
- **Immediate CI/CD Feedback**: Developers get quality gate results 7x faster
- **Cost-Effective Pipeline**: 85% reduction in CI/CD compute time and costs
- **Maintained Quality Standards**: All 241 tests pass with 67%+ coverage, Pylint 10.00/10, MyPy clean
- **Production Stability**: Zero functionality lost while achieving dramatic performance gains

## What's New in v3.1.3

### Pipeline Reliability and API Integration Improvements
- **Fail-Fast Rate Limit Handling**: Enhanced error handling with immediate pipeline failure on daily rate limits instead of silent incomplete data collection
- **FX Daily API Integration Fixed**: Corrected Alpha Vantage TIME_SERIES_FX_DAILY response key format for successful USD/GBP data fetching
- **Enhanced Error Propagation**: AlphaVantageRateLimitError → RuntimeError → Airflow task failure with clear visibility in UI
- **Daily vs Per-Minute Rate Limit Differentiation**: Smart detection of rate limit types with appropriate retry strategies

### Previous Major Improvements (v3.1.1)
- **Enhanced Makefile Architecture**: Refactored into 15+ helper functions for better organization and 5-line target compliance
- **Comprehensive Quality Pipeline**: 7-step validation (Makefile → SQL → Black → isort → Pylint → MyPy → Tests) with 100% success rate
- **Pylint Score Achievement**: Restored and maintained 10.00/10 across all modules with strategic Airflow 3.0 compatibility
- **CI/CD Workflow Optimization**: Streamlined GitHub Actions with graceful degradation and local testing via `make act-pr`
- **Test Coverage Excellence**: Maintained 69% coverage with 245+ tests passing (100% success rate)

### Technical Infrastructure Enhancements
- **Python Environment Consistency**: Fixed virtual environment path for reliable tool availability
- **Airflow 3.0.4 Compatibility**: Strategic Pylint suppressions for modern @dag and @task decorators
- **Code Quality Standards**: Black formatting, isort organization, comprehensive type checking
- **Graceful Degradation**: Optional tools (checkmake, sqlfluff) with informative fallback messaging
- **Development Workflow**: Enhanced `make all` target combining setup, installation, and quality validation

### Breaking Changes from v3.1.0
- **Makefile Structure**: Large targets refactored into focused helper functions (backward compatible)
- **Python Path**: Makefile now uses `.venv/bin/python` instead of system `python3`
- **Quality Requirements**: Enhanced Pylint configuration for DAG file compatibility

## Quick Start

### Requirements

**Python Version**: This project is built specifically for Python 3.11.12 and requires exactly this version for optimal compatibility with all dependencies and type annotations.

```bash
python --version  # Should output: Python 3.11.12
```

**System Requirements**:
- PostgreSQL (for database backend)
- Git (for version control)
- Make (for automation commands)

### Step-by-Step Setup

Follow these commands in order to set up the project:

#### 1. Clone and Enter Repository
```bash
git clone https://github.com/willdeeep/ticker-converter.git
cd ticker-converter
```

#### 2. Complete Environment Setup
```bash
make setup
# This creates .env file with default values
# Edit .env file to add your Alpha Vantage API key:
# ALPHA_VANTAGE_API_KEY=your_api_key_here
```

#### 3. Install Dependencies
Choose the installation type based on your needs:

```bash
# For complete development environment (recommended):
make all
# Combines: setup → install-test → quality validation

# Or individual installation steps:
make install        # Production dependencies only
make install-test   # Production + testing tools (recommended for development)
```

#### 4. Initialize Database
```bash
make init-db
# This sets up PostgreSQL with sample data (last 30 trading days)
```

#### 5. Start Services
```bash
# Start Apache Airflow for workflow orchestration:
make airflow
# Note: Airflow web UI will be available at http://localhost:8080

# In a separate terminal, start the FastAPI server:
make serve
# Note: API endpoints will be available at http://localhost:8000
```

### Daily Operations

#### Running Data Collection
```bash
make run  # Executes daily ETL pipeline
```

#### Testing and Quality Assurance
```bash
make test      # Run test suite with coverage (Current: 69% coverage, 245+ tests)
make test-int  # Run integration tests (external services)
make lint      # Check code quality (Pylint 10.00/10 maintained)
make lint-fix  # Auto-fix formatting issues
make quality   # Complete 7-step quality pipeline
make act-pr    # Test GitHub Actions locally
```

#### Maintenance
```bash
make clean           # Clean build artifacts
make airflow-close   # Stop Airflow services
make db-close        # Stop PostgreSQL
```

### Complete Teardown
```bash
make teardown-airflow  # Remove all Airflow data
make teardown-db       # Remove PostgreSQL database
make teardown-env      # Remove virtual environment
```

## API Endpoints

When running `make serve`, the following endpoints become available:

- **Health Check**: http://localhost:8000/health
- **API Documentation**: http://localhost:8000/docs
- **Top Performers**: http://localhost:8000/api/stocks/top-performers
- **Performance Details**: http://localhost:8000/api/stocks/performance-details

## Troubleshooting

### Common Issues and Solutions

#### Python Version Mismatch
```bash
# Verify you have Python 3.11.12
python --version

# If not, install via pyenv (recommended):
pyenv install 3.11.12
pyenv local 3.11.12
```

#### Database Connection Issues
```bash
# Check if PostgreSQL is running
make db-close  # Stop any existing instances
make init-db   # Reinitialize database
```

#### Airflow Issues
```bash
# Reset Airflow if needed
make teardown-airflow
make airflow
```

#### Import/Module Errors
```bash
# Ensure you're in the virtual environment
source .venv/bin/activate

# Reinstall dependencies
make install-dev
```

#### API Key Issues
```bash
# Verify your .env file has the API key
cat .env | grep ALPHA_VANTAGE_API_KEY

# Get a free API key at: https://www.alphavantage.co/support/#api-key
```

### Getting Help
```bash
make help  # Show all available commands with descriptions
```

## Makefile Commands

### Primary Workflow Commands
- `make all` - Complete development workflow (setup → install-test → quality)
- `make help` - Show all available commands with descriptions

### Installation Commands
- `make setup` - Initialize project with guided environment configuration
- `make install` - Install production dependencies only
- `make install-test` - Install production + testing dependencies (recommended)

### Operational Commands
- `make init-db` - Initialize database with last 30 trading days of data
- `make run` - Run daily data collection for previous trading day
- `make airflow` - Start Apache Airflow 3.0.4 instance with default user
- `make serve` - Start FastAPI development server

### Quality & Testing Commands
- `make quality` - Run comprehensive 7-step quality validation pipeline
- `make test` - Run test suite with coverage (Current: 69%, 245+ tests)
- `make lint` - Run all code quality checks (Pylint 10.00/10 maintained)
- `make lint-fix` - Auto-fix code quality issues
- `make act-pr` - Test GitHub Actions CI/CD locally
- `make clean` - Clean build artifacts and cache files

### Airflow Access
After running `make airflow`, access the web interface at:
- **URL**: http://localhost:8080
- **Username**: admin
- **Password**: admin123

## Development Environment

### Code Quality Standards
The project maintains strict code quality with:
- **pylint**: 10.00/10 scores across all modules (maintained consistently)
- **black**: Consistent code formatting (120-character line length)
- **isort**: Organized import statements with black profile
- **mypy**: Static type checking with strict settings
- **7-step quality pipeline**: Comprehensive validation from Makefile to tests
- **pre-commit**: Automated quality gates (when using install-test)

### Project Structure
```
ticker-converter/
├── src/ticker_converter/             # Core application package
│   ├── cli/                          # Command-line interface modules
│   ├── api_clients/                  # External API client classes
│   ├── data_ingestion/               # Data fetching and processing
│   ├── data_models/                  # Pydantic models and validation
│   ├── cli.py                        # Main CLI entry point
│   ├── config.py                     # Application configuration
│   └── run_api.py                    # FastAPI server launcher
├── api/                              # FastAPI application
│   ├── main.py                       # Application instance
│   ├── models.py                     # API request/response models
│   ├── database.py                   # Database connections
│   └── dependencies.py              # Dependency injection
├── dags/                             # Apache Airflow DAGs
│   ├── daily_etl_dag.py             # Main ETL workflow
│   ├── sql/                          # SQL scripts for DAGs
│   └── raw_data/                     # Data staging area
├── tests/                            # Test suite organization
│   ├── unit/                         # Unit tests (mirrors src/)
│   ├── integration/                  # Integration tests
│   ├── quality/                      # Code quality scripts
│   └── fixtures/                     # Test data and mocks
├── docs/                             # Project documentation
│   ├── architecture/                 # System design docs
│   ├── deployment/                   # Setup and deployment guides
│   └── user_guides/                  # End-user documentation
├── scripts/                          # Utility and maintenance scripts
├── sql/                              # SQL scripts and schema
│   ├── ddl/                          # Data definition (tables, views)
│   ├── etl/                          # ETL transformation scripts
│   └── queries/                      # API query templates
└── data/                             # Local data storage
```

> See individual directory README.md files for detailed organization guidelines.

### Testing Strategy
Run comprehensive tests with coverage reporting:
```bash
make test      # Runs pytest with coverage (Current: 69% coverage, 245+ tests)
make test-int  # Runs integration tests for external services
```

**Unit Testing Achievement Status**:
- ✅ **CLI Module**: 97% coverage (Phase 1 Priority 1 COMPLETED)
- ✅ **Database Manager**: 99% coverage (Phase 1 Priority 2 COMPLETED)
- ✅ **API Clients**: 85% coverage (High quality external integration)
- ✅ **Data Models**: 78% coverage (Pydantic validation complete)
- 🎯 **Next Priorities**: Orchestrator (32%), NYSE Fetcher (20%)
- 📊 **Overall Progress**: 69% coverage maintained consistently

**Integration Testing Coverage**:
- ✅ **Alpha Vantage API**: Connectivity, authentication, data format validation
- ✅ **PostgreSQL Database**: Connection, permissions, schema validation  
- ✅ **Apache Airflow 3.0.4**: Configuration, DAG validation, modern @task decorators
- ✅ **FastAPI Endpoints**: Application startup, data serialization, error handling
- ✅ **Quality Pipeline**: 7-step validation with 100% success rate

Coverage reports are generated in `htmlcov/` for detailed analysis.
Integration test documentation available in `docs/user_guides/integration_testing.md`.

## Installation Options

### Production Environment
```bash
make install  # Core dependencies only
```

### Testing Environment  
```bash
make install-test  # Includes pytest, coverage tools
```

### Development Environment
```bash
make install-dev  # Full toolchain with quality tools
```

## Dependencies

### Core Dependencies
- FastAPI >= 0.104.0 (Web framework)
- uvicorn >= 0.24.0 (ASGI server)
- psycopg2-binary >= 2.9.7 (PostgreSQL adapter)
- pandas >= 2.1.0 (Data processing)
- Apache Airflow >= 3.0.4 (Workflow orchestration)

### Development Tools
- pylint >= 3.0.0 (Code analysis)
- black >= 23.0.0 (Code formatting)
- isort >= 5.12.0 (Import sorting)
- mypy >= 1.5.0 (Type checking)
- pre-commit >= 3.4.0 (Git hooks)

## Code Quality

The project enforces high code quality standards:

### Automated Formatting
```bash
make lint-fix  # Apply black + isort formatting
```

### Quality Checks
```bash
make lint  # Run pylint, black, isort, mypy
```

### Pre-commit Hooks
Installed automatically with `make install-dev`:
- Trailing whitespace removal
- YAML syntax checking
- Code formatting with black
- Import sorting with isort
- Linting with pylint

## Architecture

### SQL-First Design
- Custom SQL queries optimized for financial data analysis
- PostgreSQL-only architecture for data persistence
- Async database connections for optimal performance

### API Design
- RESTful endpoints with automatic OpenAPI documentation
- Pydantic models for type-safe request/response handling
- Comprehensive error handling with detailed HTTP responses

### Data Pipeline
- Apache Airflow orchestration for reliable data workflows
- Alpha Vantage API integration for market data
- Magnificent Seven stock focus (AAPL, MSFT, GOOGL, etc.)

## Production Deployment

### Environment Variables
Set up your `.env` file:
```bash
ALPHA_VANTAGE_API_KEY=your_api_key
DATABASE_URL=postgresql://user:pass@localhost:5432/ticker_converter
```

### Database Setup
The `make init-db` command handles:
- Schema creation
- Historical data population
- Index optimization
- View generation

### Service Management
- **FastAPI**: `make serve` starts development server
- **Airflow**: `make airflow` starts full Airflow stack
- **Database**: PostgreSQL connection handling built-in

## Contributing

### Development Workflow
1. Fork the repository
2. Create feature branch (`git checkout -b feature/name`)
3. Install development environment (`make install-dev`)
4. Make changes with quality checks (`make lint-fix`)
5. Run tests (`make test`)
6. Commit changes (pre-commit hooks apply automatically)
7. Push and create Pull Request

### Quality Standards
All contributions must:
- Pass `make lint` with 10/10 pylint scores
- Include tests with `make test` passing
- Follow black formatting standards
- Include type hints for mypy

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Support

- **Documentation**: [docs/](docs/)
- **Issues**: [GitHub Issues](https://github.com/willdeeep/ticker-converter/issues)
- **API Docs**: http://localhost:8000/docs (when running `make serve`)

## Further Reading

Explore detailed documentation in the `docs/` folder:

### Architecture Documentation
- [System Overview](docs/architecture/overview.md) - Strategic system architecture and technology decisions
- [ETL Pipeline Implementation](docs/architecture/etl_pipeline_implementation.md) - Detailed ETL pipeline mechanics and data flow
- [Database Schema and Operations](docs/architecture/database_schema_and_operations.md) - Schema design with explicit normalization strategy
- [Technology Choices](docs/architecture/technology_choices.md) - Technology selection analysis and decision rationale
- [Airflow Setup](docs/architecture/airflow_setup.md) - Apache Airflow 3.0.4 configuration and modern patterns
- [API Design](docs/architecture/api_design.md) - FastAPI endpoint specifications and architectural decisions

### Deployment Guides
- [Local Setup](docs/deployment/local_setup.md) - Comprehensive local development environment setup
- [Production Deployment](docs/deployment/production.md) - Enterprise-grade production deployment procedures

### User Guides
- [CLI Usage](docs/user_guides/cli_usage.md) - Complete CLI command reference and development workflows

---

**Ready for production deployment with comprehensive tooling and quality standards!**
